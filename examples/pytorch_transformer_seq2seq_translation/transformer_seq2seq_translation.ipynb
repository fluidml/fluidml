{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer based Sequence to Sequence Translation using FluidML\n",
    "In this notebook, we utilize FluidML to implement a complete ML pipeline that performs text translation from German to English.  \n",
    "Our translation pipeline consists of the following tasks:\n",
    "- **Dataset loading**: Downloads and parses the Multi30K dataset used for translation.\n",
    "- **Tokenizer training**: Trains and saves a Byte Pair Encoding (BPE) Tokenizer used for text encoding and decoding.\n",
    "- **Dataset encoding**: Encodes the dataset with the trained BPE Tokenizer.\n",
    "- **Model training**: Trains the Transformer based Sequence to Sequence Model.\n",
    "- **Model selection**: Selects from all trained model variations (different hyperparameter sweeps) the best performing one based on the validation set.\n",
    "- **Model evaluation**: Evaluates the best performing model on the test set by calculating the test loss and the bleu score.\n",
    "\n",
    "With FluidML, all of these steps are naturally implemented as individual tasks which register their dependencies and are chained together to a task graph. This graph is then executed in parallel by FluidML and all results are stored persistently in a local file store (see the Storage section for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this example it makes sense to install FluidML with the additional example requirements (Of course you can also manually install all dependencies. Check `transformer_seq2seq_translation.py` for a complete list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fluidml[examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Due to the limitation of multiprocessing and jupyter, we have to import our defined tasks and some helper classes from a separate script. Hence, our task definitions are located in `transformer_seq2seq_translation.py`, which not only implements the tasks but also the entire functionality of this example. So the interested reader can also go ahead and execute the just mentioned script. In order to still make this notebook self-explanatory, we provide Markdown code snippets of the individual task implementations at the place where we would have defined the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python internal imports\n",
    "import multiprocessing\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# External imports\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# FluidML imports\n",
    "from fluidml import Task, Flow, Swarm\n",
    "from fluidml.common import Resource\n",
    "from fluidml.flow import GridTaskSpec, TaskSpec\n",
    "from fluidml.visualization import visualize_graph_in_console\n",
    "\n",
    "# Task imports, file store import and resource class import (see above note)\n",
    "from transformer_seq2seq_translation import DatasetLoading, TokenizerTraining, DatasetEncoding, Training, ModelSelection, Evaluation\n",
    "from transformer_seq2seq_translation import TaskResource, MyLocalFileStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2**: If you want to use FluidML's logging capability, please configure a logger using Python's `logging` API. For convenience, we provide a simple utility function which configures a visually appealing logger (using a specific handler from the `rich` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluidml.common.logging import configure_logging\n",
    "configure_logging(level='INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage - Saving Objects with FluidML\n",
    "\n",
    "Before we start with the implementation of our translation pipeline, we take a brief look at FluidML's storage API.  \n",
    "Out of the box FluidML provides three different storage options, which share the same interface (the interested user can implement his own storage, as long as the storage class inherits from `fluidml.storage.base.ResultsStore` and implements all abstract methods):\n",
    "- **InMemoryStore**: If no store object is provided, this is internally the default. Every saved object is stored in an in-memory manager dictionary, which is shared across all tasks and processes. Once the entire pipeline is executed, the results dictionary is returned to the user and it is the user's responsibility to actually save the results e.g. to disc. This store is only recommended for quick prototyping and small result objects, since intermediate task results cannot be stored persistently and the memory might not be sufficient to hold all task results.\n",
    "- **LocalFileStore**: A persistent file store implementation that out of the box supports saving files as .json and .pickle. It can be easily extended by the user to support arbitrary file types and save options.\n",
    "- **MongoDBStore**: A persistent MongoDBStore implementation, which stores saved objects as binary strings via GridFS in a Mongo DB.\n",
    "\n",
    "In this example we utilize the `LocalFileStore` and extend it with our own custom saving types.  \n",
    "In order to save an object within a task, one simply calls\n",
    "```python\n",
    "self.save(obj=model_state_dict, name='best_model', type_='torch', sub_dir='models')\n",
    "self.save(obj=some_dict, name='some_dict', type_='json')\n",
    "self.save(obj=some_serializable_obj, name='some_serializable_obj', type_='pickle', sub_dir='some/sub/dir')\n",
    "```\n",
    "Below, we implement `MyLocalFileStore`, which inherits from `LocalFileStore` and extends it by adding save and load functions for torch models and tokenizer objects. \n",
    "We register new types to the `_type_registry` dictionary from `LocalFileStore` by providing a `TypeInfo` dataclass instance containing the type's save and load function and file extension.\n",
    "\n",
    "```python\n",
    "from fluidml.storage import LocalFileStore, TypeInfo\n",
    "\n",
    "\n",
    "class MyLocalFileStore(LocalFileStore):\n",
    "    def __init__(self, base_dir: str):\n",
    "        super().__init__(base_dir=base_dir)\n",
    "\n",
    "        self._type_registry['torch'] = TypeInfo(self._save_torch, self._load_torch, 'pt')\n",
    "        self._type_registry['tokenizer'] = TypeInfo(self._save_tokenizer, self._load_tokenizer, 'json')\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_torch(name: str, obj: Any, obj_dir: str, extension: str):\n",
    "        torch.save(obj, f=os.path.join(obj_dir, f'{name}.{extension}'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_torch(name: str, obj_dir: str, extension: str) -> Any:\n",
    "        return torch.load(os.path.join(obj_dir, f'{name}.{extension}'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_tokenizer(name: str, obj: Tokenizer, obj_dir: str, extension: str):\n",
    "        obj.save(os.path.join(obj_dir, f'{name}.{extension}'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_tokenizer(name: str, obj_dir: str, extension: str) -> Tokenizer:\n",
    "        return Tokenizer.from_file(os.path.join(obj_dir, f'{name}.{extension}'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Definitions\n",
    "\n",
    "The following 6 sections describe our task definitions in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading\n",
    "\n",
    "For this example we use the [Multi30K](https://github.com/multi30k/dataset) translation dataset, considering only German and English. The dataset was published with a fixed split of 29,000 train, 1,000 validation and 1,000 test German-English text pairs.\n",
    "\n",
    "We implement this Task by creating a custom `DatasetLoading` class, which inherits from FluidML's `Task` class. To comply with our interface a custom task just has to implement a `run()` method, which FluidML will execute internally.\n",
    "\n",
    "Here is our complete implementation of DatasetLoading (imported above):\n",
    "\n",
    "\n",
    "```python\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "\n",
    "class DatasetLoading(Task):\n",
    "    def __init__(self,\n",
    "                 base_url: str,\n",
    "                 data_split_names: Dict[str, List]):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.data_split_names = data_split_names\n",
    "\n",
    "    @staticmethod\n",
    "    def download_and_extract_gz_from_url(url: str) -> List[str]:\n",
    "        # download gz compressed data\n",
    "        data_gz = requests.get(url=url)\n",
    "        # decompress downloaded gz data to bytes object\n",
    "        data_bytes = gzip.decompress(data_gz.content)\n",
    "        # decode bytes object to utf-8 encoded str and convert to list by splitting on new line chars\n",
    "        data = data_bytes.decode('utf-8').splitlines()\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "        task_dir = self.results_store.get_context(task_name=self.name, task_unique_config=self.unique_config)\n",
    "        task_dir = os.path.relpath(task_dir, self.results_store.base_dir)\n",
    "        logger.info(f'Download and save raw dataset to \"{task_dir}\".')\n",
    "        \n",
    "        for split_name, files in self.data_split_names.items():\n",
    "            dataset = {}\n",
    "            for file_name in files:\n",
    "                # create download url\n",
    "                url = self.base_url + file_name\n",
    "                language = file_name.split('.')[1]\n",
    "                # download and parse data\n",
    "                data = DatasetLoading.download_and_extract_gz_from_url(url=url)\n",
    "                dataset[language] = data\n",
    "            # save train-, valid- and test-data as json via local file store\n",
    "            self.save(obj=dataset, name=f'{split_name}_data', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizer Training\n",
    "\n",
    "We utilize Huggingface's [Tokenizers](https://github.com/huggingface/tokenizers) library to train a Byte Pair Encoding (BPE) tokenizer for the German and for the English sentences.\n",
    "\n",
    "\n",
    "```python\n",
    "from tokenizers.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class TokenizerTraining(Task):\n",
    "    def __init__(self, vocab_size: int, min_frequency: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "\n",
    "    def train_tokenizer(self, data: List[str]):\n",
    "        # initialize and train a tokenizer\n",
    "        tokenizer = CharBPETokenizer()\n",
    "        tokenizer.train_from_iterator(iterator=data,\n",
    "                                      vocab_size=self.vocab_size,\n",
    "                                      min_frequency=self.min_frequency,\n",
    "                                      special_tokens=['<unk>', '<bos>', '<eos>', '<pad>'],\n",
    "                                      show_progress=True)\n",
    "        \n",
    "        # add template rule to automatically add <bos> and <eos> to the encoding\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            pair=None,\n",
    "            special_tokens=[\n",
    "                (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "            ],\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "    def run(self, train_data: Dict[str, List[str]]):\n",
    "        task_dir = self.results_store.get_context(task_name=self.name, task_unique_config=self.unique_config)\n",
    "        task_dir = os.path.relpath(task_dir, self.results_store.base_dir)\n",
    "        \n",
    "        # train german tokenizer\n",
    "        de_tokenizer = self.train_tokenizer(data=train_data['de'])\n",
    "\n",
    "        # train english tokenizer\n",
    "        en_tokenizer = self.train_tokenizer(data=train_data['en'])\n",
    "\n",
    "        # save tokenizers\n",
    "        logger.info(f'Save trained tokenizers to \"{task_dir}\".')\n",
    "        self.save(obj=de_tokenizer, name='de_tokenizer', type_='tokenizer')\n",
    "        self.save(obj=en_tokenizer, name='en_tokenizer', type_='tokenizer')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset Encoding\n",
    "\n",
    "We use the trained tokenizers for German and English to encode the previously saved datasets and save them as json files.  \n",
    "**Note**: FluidML automatically collects the required task inputs from the saved predecessor task results; in this case the datasets saved from `DatasetLoading` and the tokenizers trained in `TokenizerTraining`.\n",
    "\n",
    "\n",
    "```python\n",
    "class DatasetEncoding(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_data(data: Dict[str, List[str]],\n",
    "                    src_tokenizer: Tokenizer,\n",
    "                    trg_tokenizer: Tokenizer) -> List[Tuple[List[int], List[int]]]:\n",
    "\n",
    "        src_encoded = src_tokenizer.encode_batch(data['de'])\n",
    "        trg_encoded = trg_tokenizer.encode_batch(data['en'])\n",
    "        return [(src.ids, trg.ids) for src, trg in zip(src_encoded, trg_encoded)]\n",
    "\n",
    "    def run(self,\n",
    "            train_data: Dict[str, List[str]],\n",
    "            valid_data: Dict[str, List[str]],\n",
    "            test_data: Dict[str, List[str]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "        \n",
    "        task_dir = self.results_store.get_context(task_name=self.name, task_unique_config=self.unique_config)\n",
    "        task_dir = os.path.relpath(task_dir, self.results_store.base_dir)\n",
    "\n",
    "        train_encoded = DatasetEncoding.encode_data(train_data, de_tokenizer, en_tokenizer)\n",
    "        valid_encoded = DatasetEncoding.encode_data(valid_data, de_tokenizer, en_tokenizer)\n",
    "        test_encoded = DatasetEncoding.encode_data(test_data, de_tokenizer, en_tokenizer)\n",
    "\n",
    "        logger.info(f'Save encoded dataset to \"{task_dir}\".')\n",
    "        self.save(obj=train_encoded, name='train_encoded', type_='json')\n",
    "        self.save(obj=valid_encoded, name='valid_encoded', type_='json')\n",
    "        self.save(obj=test_encoded, name='test_encoded', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training\n",
    "\n",
    "Our dataset is encoded using the trained BPE tokenizers, so the next step is to train the actual translation model. We utilize the well known transformer architecture first described in the [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper. Both, the encoder and decoder consist of several multi-head attention layers, which are the backbone of this architecture. Since this tutorial is about FluidML, we won't go into the implementational details of a transformer model. We refer the interested reader to [Ben Trevett's tutorial on transformers for translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb). Also, the pytorch transformer implementation used in this example is taken from Ben's tutorial.\n",
    "\n",
    "**Note**: The custom pytorch dataset, `TranslationDataset`, and the batch collate callable, `BatchCollator`, implementations can be also found in the above mentioned script, `transformer_seq2seq_translation.py`, from where the task classes are imported. Due to its complexity the transformer model implementation is also imported in this notebook and can be found in the `transformer_model.py` script. The definition of the `set_seed()` function is also located in `transformer_seq2seq_translation.py`.\n",
    "\n",
    "You also might note that we use `self.resource.device` in our training task without explicitly defining it in the `__init__()` method. When initializing FluidML's main classes `Flow` and `Swarm`, the user can optionally provide a list of resources that will be made available to all tasks and processes. In a machine learning context such resources could be but are not limited to cuda devices (see this example). Below we will go through how to define and provide the list of resources to `Flow` and how the resources will be automatically distributed to tasks.\n",
    "\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Training(Task):\n",
    "    def __init__(self,\n",
    "                 hid_dim: int,\n",
    "                 enc_layers: int,\n",
    "                 dec_layers: int,\n",
    "                 enc_heads: int,\n",
    "                 dec_heads: int,\n",
    "                 enc_pf_dim: int,\n",
    "                 dec_pf_dim: int,\n",
    "                 enc_dropout: float,\n",
    "                 dec_dropout: float,\n",
    "                 learning_rate: float,\n",
    "                 clip_grad: float,\n",
    "                 train_batch_size: int,\n",
    "                 valid_batch_size: int,\n",
    "                 num_epochs: int,\n",
    "                 seed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # transformer model parameters\n",
    "        self.hid_dim = hid_dim\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.enc_heads = enc_heads\n",
    "        self.dec_heads = dec_heads\n",
    "        self.enc_pf_dim = enc_pf_dim\n",
    "        self.dec_pf_dim = dec_pf_dim\n",
    "        self.enc_dropout = enc_dropout\n",
    "        self.dec_dropout = dec_dropout\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_grad = clip_grad\n",
    "\n",
    "        # dataloader and training loop parameters\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.seed = seed\n",
    "\n",
    "    def _init_training(self, input_dim: int, output_dim: int, src_pad_idx: int, trg_pad_idx: int):\n",
    "        \"\"\" Initialize all training components.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize the encoder and decoder block\n",
    "        enc = Encoder(input_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.enc_layers,\n",
    "                      self.enc_heads,\n",
    "                      self.enc_pf_dim,\n",
    "                      self.enc_dropout,\n",
    "                      self.resource.device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.dec_layers,\n",
    "                      self.dec_heads,\n",
    "                      self.dec_pf_dim,\n",
    "                      self.dec_dropout,\n",
    "                      self.resource.device)\n",
    "\n",
    "        # initialize the full transformer sequence to sequence model\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, self.resource.device).to(self.resource.device)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # initialize the loss criterion\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "        return model, optimizer, criterion\n",
    "\n",
    "    def _train_epoch(self, model, iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "            trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_epoch(model, iterator, criterion):\n",
    "        \"\"\" Validation loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, trg in iterator:\n",
    "\n",
    "                output, _ = model(src, trg[:, :-1])\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "                trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "                epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    def _train(self, model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop.\n",
    "        \"\"\"\n",
    "        task_dir = self.results_store.get_context(task_name=self.name, task_unique_config=self.unique_config)\n",
    "        task_dir = os.path.relpath(task_dir, self.results_store.base_dir)\n",
    "        model_dir = os.path.join(task_dir, 'models')\n",
    "        logger.info(f'Save model checkpoints to \"{model_dir}\".')\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            start_time = datetime.now()\n",
    "            train_loss = self._train_epoch(model, train_iterator, optimizer, criterion)\n",
    "            valid_loss = Training.validate_epoch(model, valid_iterator, criterion)\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            # if the current validation loss is below the previous best, update the best loss and\n",
    "            # save the new best model.\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model = model.state_dict()\n",
    "                self.save(obj=best_model, name='best_model', type_='torch')\n",
    "                self.save(obj={'epoch': epoch,\n",
    "                               'valid_loss': best_valid_loss}, name='best_model_metric', type_='json')\n",
    "\n",
    "            logger.info(f'\\nEpoch: {epoch + 1:02} | Time: {end_time - start_time}\\n'\n",
    "                        f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\\n'\n",
    "                        f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "        assert best_model is not None\n",
    "        return best_model, best_valid_loss\n",
    "\n",
    "    def run(self,\n",
    "            train_encoded: List[Tuple[List[int], List[int]]],\n",
    "            valid_encoded: List[Tuple[List[int], List[int]]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "        set_seed(self.seed)\n",
    "\n",
    "        # instantiate the collate fn for the dataloader\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "\n",
    "        # instantiate train and validation datasets using a pytorch's Dataset class\n",
    "        train_dataset = TranslationDataset(data=train_encoded)\n",
    "        valid_dataset = TranslationDataset(data=valid_encoded)\n",
    "\n",
    "        # instantiate train and validation dataloader\n",
    "        train_iterator = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True,\n",
    "                                    collate_fn=batch_collator)\n",
    "        valid_iterator = DataLoader(valid_dataset, batch_size=self.valid_batch_size, shuffle=False,\n",
    "                                    collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        # instantiate all training components\n",
    "        model, optimizer, criterion = self._init_training(input_dim=input_dim,\n",
    "                                                          output_dim=output_dim,\n",
    "                                                          src_pad_idx=src_pad_idx,\n",
    "                                                          trg_pad_idx=trg_pad_idx)\n",
    "\n",
    "        # train the model on the training set and evaluate after every epoch on the validation set\n",
    "        self._train(model=model,\n",
    "                    train_iterator=train_iterator,\n",
    "                    valid_iterator=valid_iterator,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Selection\n",
    "\n",
    "We have not yet talked about grid search and training several models with different hyperparameter combinations in parallel. FluidML provides a simple interface to allow just that, which we will describe further below when instantiating our tasks. For now let's assume that the previous tasks might have been executed multiple times with a set of different parameter combinations yielding several trained model variations.\n",
    "\n",
    "This task is a so called `reduce=True` task (will be used when instantiating the task), which means that it collects the results from all predecessor variations in order to compare and select the best performing variation. In this example it selects the best model variation based on the validation loss performance. \n",
    "\n",
    "**Note**: The `run` method of a reduce task expects a single input object, always named `reduced_results`, which is assembled by fluidml from all collected predecessor results.\n",
    "\n",
    "\n",
    "```python\n",
    "class ModelSelection(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _select_best_model_from_sweeps(training_results: List[Dict]) -> Dict:\n",
    "        config = None\n",
    "        best_valid_loss = float('inf')\n",
    "        for sweep in training_results:\n",
    "            if sweep['result']['best_model_metric']['valid_loss'] <= best_valid_loss:\n",
    "                best_valid_loss = sweep['result']['best_model_metric']['valid_loss']\n",
    "                config = sweep['config']\n",
    "        return config\n",
    "\n",
    "    def run(self, reduced_results: List[Dict]):\n",
    "        task_dir = self.results_store.get_context(task_name=self.name, task_unique_config=self.unique_config)\n",
    "        task_dir = os.path.relpath(task_dir, self.results_store.base_dir)\n",
    "        \n",
    "        # select the best run config by comparing model performances from different parameter sweeps \n",
    "        # on the validation set\n",
    "        best_run_config = self._select_best_model_from_sweeps(training_results=reduced_results)\n",
    "\n",
    "        logger.info(f'Save best run config to \"{task_dir}\".')\n",
    "        self.save(obj=best_run_config, name='best_run_config', type_='json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Evaluation\n",
    "\n",
    "This is the final task in our pipeline. It expects the previously determined `best_run_config` dictionary as input, loads the corresponding best model and tokenizers and evaluates said model on the test dataset.  \n",
    "First, we calculate the test set loss and perplexity. Second, we calculate the test set bleu score, since this is the standard metric of evaluating machine translation models.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class Evaluation(Task):\n",
    "    def __init__(self, test_batch_size: int, seed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = test_batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def _init_model(self, train_config: Dict, input_dim: int, output_dim: int,\n",
    "                    src_pad_idx: int, trg_pad_idx: int) -> nn.Module:\n",
    "        \"\"\" Initialize the model and its components.\n",
    "        \"\"\"\n",
    "\n",
    "        enc = Encoder(input_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['enc_layers'],\n",
    "                      train_config['enc_heads'],\n",
    "                      train_config['enc_pf_dim'],\n",
    "                      train_config['enc_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['dec_layers'],\n",
    "                      train_config['dec_heads'],\n",
    "                      train_config['dec_pf_dim'],\n",
    "                      train_config['dec_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, self.resource.device).to(self.resource.device)\n",
    "        return model\n",
    "\n",
    "    def translate_sentence(self, src_encoded, bos_idx, eos_idx, model, max_len=50):\n",
    "        \"\"\" Translate an encoded sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        src_tensor = torch.LongTensor(src_encoded).unsqueeze(0).to(self.resource.device)\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "        trg_indices = [bos_idx]\n",
    "        for i in range(max_len):\n",
    "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(self.resource.device)\n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "            with torch.no_grad():\n",
    "                output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == eos_idx:\n",
    "                break\n",
    "\n",
    "        return trg_indices[1:]\n",
    "\n",
    "    def calculate_bleu(self, data_encoded, en_tokenizer, model, max_len=50):\n",
    "        \"\"\" Calculate the bleu score on the test set.\n",
    "        \"\"\"\n",
    "\n",
    "        trgs = []\n",
    "        pred_trgs = []\n",
    "        bos_idx = en_tokenizer.token_to_id('<bos>')\n",
    "        eos_idx = en_tokenizer.token_to_id('<eos>')\n",
    "\n",
    "        worker_name = multiprocessing.current_process().name\n",
    "        with tqdm(desc=f'{worker_name} - Calculating BLEU',\n",
    "                  total=len(data_encoded),\n",
    "                  unit='sample',\n",
    "                  ascii=False,\n",
    "                  ) as progress_bar:\n",
    "            for src, trg in data_encoded:\n",
    "\n",
    "                pred_trg = self.translate_sentence(src, bos_idx, eos_idx, model, max_len)\n",
    "\n",
    "                # cut off <eos> token\n",
    "                pred_trg = pred_trg[:-1]\n",
    "\n",
    "                pred_trg_decoded = en_tokenizer.decode(pred_trg)\n",
    "                pred_trgs.append(pred_trg_decoded.split())\n",
    "\n",
    "                trg_decoded = en_tokenizer.decode(trg)\n",
    "                trgs.append([trg_decoded.split()])\n",
    "                progress_bar.update()\n",
    "\n",
    "        return bleu_score(pred_trgs, trgs)\n",
    "\n",
    "    def run(self, best_run_config: Dict):\n",
    "        set_seed(self.seed)\n",
    "\n",
    "        # load the best model, test-data and the tokenizers based on the previously selected best run config\n",
    "        model_state_dict = self.load(name='best_model', task_name='Training', task_unique_config=best_run_config)\n",
    "        test_encoded = self.load(name='test_encoded', task_name='DatasetEncoding', task_unique_config=best_run_config)\n",
    "        de_tokenizer = self.load(name='de_tokenizer', task_name='TokenizerTraining', task_unique_config=best_run_config)\n",
    "        en_tokenizer = self.load(name='en_tokenizer', task_name='TokenizerTraining', task_unique_config=best_run_config)\n",
    "\n",
    "        # instantiate the batch collator\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "\n",
    "        # instantiate the test dataset\n",
    "        test_dataset = TranslationDataset(data=test_encoded)\n",
    "\n",
    "        # instantiate the test dataloader\n",
    "        test_iterator = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        # instantiate the transformer model\n",
    "        model = self._init_model(train_config=best_run_config['Training'],\n",
    "                                 input_dim=input_dim,\n",
    "                                 output_dim=output_dim,\n",
    "                                 src_pad_idx=src_pad_idx,\n",
    "                                 trg_pad_idx=src_pad_idx)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "\n",
    "        # instantiate the loss criterion\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "        # evaluate the model on the test set -> calculate the test set loss and perplexity\n",
    "        test_loss = Training.validate_epoch(model=model, iterator=test_iterator, criterion=criterion)\n",
    "        logger.info(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "        # calculate the model's bleu score on the test set\n",
    "        bleu = self.calculate_bleu(test_encoded, en_tokenizer, model)\n",
    "        logger.info(f'BLEU score = {bleu * 100:.2f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Pipeline/Task-Graph via FluidML\n",
    "\n",
    "So far, we have looked into implementing our individual pipeline steps using FluidML's Task class and it was very straightforward.\n",
    "You might be wondering, how to put these tasks together and make them work together as a single pipeline?\n",
    "\n",
    "Thanks to FluidML's TaskSpec API, you can connect these tasks like Lego blocks :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiate Task Specs\n",
    "`TaskSpec` and `GridTaskSpec` are simple wrapper classes that allow to specify task details and task arguments which will be used during instantiation of the task.\n",
    "Let's go ahead and create specs for all our tasks.  \n",
    "\n",
    "**Note 1**: If a task publishes result objects, that are relevent to successor tasks, these object names have to be registered through the `publishes` attribute. This is necessary because based on this information FluidML decides whether a task still needs to be executed or whether it has been executed already in a previous run and all results can be loaded from the store.\n",
    "\n",
    "**Note 2**: A task where no hyperparameter tuning or grid search is necessary is wrapped in the `TaskSpec` class. Likewise, tasks, where we want to perform hyperparameter tuning, e.g. `TokenizerTraining` and `Training`, are wrapped in the dedicated `GridTaskSpec` class.\n",
    "\n",
    "Below we define for each task the necessary parameter dictionary which we feed into the task spec class.  \n",
    "\n",
    "**Note 3**: All parameters of a `GridTaskSpec` task that are stored in a list, will be automatically expanded by `Flow` depending on the selected grid search expansion method. Namely, `gs_expansion_method='product'` (default) will create different task instances for each explicit cross-product parameter combination. For example, considering the `Training` task, internally flow will instantiate 4 train tasks with the cross product combinations of different `train_batch_size` and `learning_rate`. Alternatively, `gs_expansion_method='zip'` expands the config parameters by zipping over the different parameter lists. For example, `config = {'learning_rate': [0.1, 0.01, 0.001], 'batch_size': [64, 128, 256]}` would expand to three distinct configs with the respective learning rates and batch sizes of `(0.1, 64), (0.01, 128), (0.001, 256)`. When choosing `zip` one has to make sure that all parameter lists are of equal lengths. \n",
    "\n",
    "**Note 4**: If you have to provide a parameter to a `GridTaskSpec` task, which is of type `List` so it should not get expanded, you have to wrap it again in a second list. E.g. `layer_dimensions: [[64, 128, 64]]`.\n",
    "\n",
    "**Note 5**: The `ModelSelection` task is a so called `reduce=True` task, which means it receives the combined input of all expanded direct predecessor tasks (in this case 4 train tasks). If not all direct predecessor task output objects are required, it makes sense to specify the required objects in the `expects` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "\n",
    "dataset_loading_params = {'base_url': 'https://raw.githubusercontent.com/multi30k/dataset/'\n",
    "                                      'master/data/task1/raw/',\n",
    "                          'data_split_names': {'train': ['train.de.gz', 'train.en.gz'],\n",
    "                                               'valid': ['val.de.gz', 'val.en.gz'],\n",
    "                                               'test': ['test_2016_flickr.de.gz', 'test_2016_flickr.en.gz']}}\n",
    "dataset_loading_task = TaskSpec(task=DatasetLoading, config=dataset_loading_params, publishes=['train_data', 'valid_data', 'test_data'])\n",
    "\n",
    "\n",
    "tokenizer_training_params = {'vocab_size': 30000,\n",
    "                             'min_frequency': 2}\n",
    "tokenizer_training_task = TaskSpec(task=TokenizerTraining, config=tokenizer_training_params, publishes=['de_tokenizer', 'en_tokenizer'])\n",
    "\n",
    "\n",
    "dataset_encoding_task = TaskSpec(task=DatasetEncoding, publishes=['train_encoded', 'valid_encoded', 'test_encoded'])\n",
    "\n",
    "\n",
    "training_params = {'hid_dim': 256,\n",
    "                   'enc_layers': 3,\n",
    "                   'dec_layers': 3,\n",
    "                   'enc_heads': 8,\n",
    "                   'dec_heads': 8,\n",
    "                   'enc_pf_dim': 512,\n",
    "                   'dec_pf_dim': 512,\n",
    "                   'enc_dropout': 0.1,\n",
    "                   'dec_dropout': 0.1,\n",
    "                   'learning_rate': [0.0005, 0.001],\n",
    "                   'clip_grad': 1.,\n",
    "                   'train_batch_size':[64, 128],\n",
    "                   'valid_batch_size': 128,\n",
    "                   'num_epochs': 8,\n",
    "                   'seed': 1234}\n",
    "train_task = GridTaskSpec(task=Training, gs_config=training_params, gs_expansion_method='product', publishes=['best_model', 'best_model_metric'])\n",
    "\n",
    "\n",
    "model_selection_task = TaskSpec(task=ModelSelection, reduce=True, expects=['best_model_metric'],\n",
    "                                publishes=['best_run_config'])\n",
    "\n",
    "\n",
    "evaluation_params = {'test_batch_size': 128,\n",
    "                     'seed': 1234}\n",
    "evaluate_task = TaskSpec(task=Evaluation, config=evaluation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Registering all Task Dependencies\n",
    "After having instantiated all task specs, we utilize the `requires()` method to register dependencies between tasks.\n",
    "\n",
    "Using these task dependencies, FluidML's `Flow` class properly expands all `GridTaskSpecs` and creates a task graph. Next, FluidML's `Swarm` class schedules and performs the parallel task executions considering the registered dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dependencies between tasks\n",
    "\n",
    "tokenizer_training_task.requires(dataset_loading_task)\n",
    "dataset_encoding_task.requires([tokenizer_training_task, dataset_loading_task])\n",
    "train_task.requires([dataset_encoding_task, tokenizer_training_task])\n",
    "model_selection_task.requires(train_task)\n",
    "evaluate_task.requires(model_selection_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the List of Task Spec Instances\n",
    "We pack all these task specs in a list which gets passed to FluidML. `Flow` internally creates the task instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [dataset_loading_task, tokenizer_training_task, dataset_encoding_task, train_task, model_selection_task, evaluate_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setting all Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically get the directory of this script\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "# define the base directory where all our task results will be stored in a structured way using LocalFileStore\n",
    "base_dir = os.path.join(current_dir, 'seq2seq_experiments')\n",
    "\n",
    "# select the number of workers (processes used to execute tasks in parallel)\n",
    "num_workers = 4\n",
    " \n",
    "# set force to \n",
    "#  1) 'all' if all tasks in the pipeline have to be force-executed.\n",
    "#  2) None if already existing tasks are skipped so that results can be loaded from the store.\n",
    "#  3) a task name (eg. \"PreProcessTask\") or list of task names (eg. [\"PreProcessTask1\", \"PreProcessTask2])\n",
    "#     Additionally, each task name can have the suffix '+' to re-run also its successors (eg. \"PreProcessTask+\")\n",
    "force = None\n",
    "\n",
    "# try to use cuda GPU's if available\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define and instantiate Resources to share across all Tasks\n",
    "\n",
    "We mentioned already during the `Training` task that FluidML enables the user to conveniently share resources across all tasks instead of providing them explicitely to each task individually.  \n",
    "The user achieves this by creating his own Resource dataclass, which inherits from our `Resource` interface. In this dataclass we define all resources, in our case only the cuda device, which we make available to all tasks through the `self.resource` attribute.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TaskResource(Resource):\n",
    "    device: str\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we utilize a little helper function to distribute our available cuda devices euqally across the number of defined workers.  \n",
    "E.g. let's assume we selected `num_workers = 4` and we have access to two GPU's, the below function would return the following balanced list of devices:\n",
    "\n",
    "```python\n",
    "print(devices)\n",
    "-> ['cuda:0', 'cuda:1', 'cuda:0', 'cuda:1']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_devices(count: Optional[int] = None,\n",
    "                         use_cuda: bool = True) -> List[str]:\n",
    "    count = count if count is not None else multiprocessing.cpu_count()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        devices = [f'cuda:{id_}' for id_ in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        devices = ['cpu']\n",
    "    factor = int(count / len(devices))\n",
    "    remainder = count % len(devices)\n",
    "    devices = devices * factor + devices[:remainder]\n",
    "    return devices\n",
    "\n",
    "devices = get_balanced_devices(count=num_workers, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our list of resource objects which we will feed to the `Swarm` class during instantiation.  \n",
    "**Note**: `len(resources) == num_workers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of resources\n",
    "resources = [TaskResource(device=devices[i]) for i in range(num_workers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Instantiate the previously defined File Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local file storage used for versioning\n",
    "results_store = MyLocalFileStore(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run the Pipeline/Task-Graph\n",
    "\n",
    "We use a context manager to instantiate the `Swarm` class which handles executing the created task graph in parallel. Next, we instantiate a flow instance by passing the swarm to the `Flow` class. Flow handles the graph creation and graph expansion logic. We create a \"flow\" by calling `flow.create()` which builds the user defined task specifier graph and the expanded task graph. Both can be easily accessed as attributes and visualized using our console rendering utility function. Finally, we execute the expanded task graph by calling `flow.run()`.  \n",
    "\n",
    "All saved task results will be availabe in the previously selected output directory (if not changed: `seq2seq_experiments`).  \n",
    "Feel free to play around with different hyperparameter combinations to improve your model's translation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:52] </span><span style=\"color: #800000\">WARNING </span> MainProcess  Console does not support paging. Defaulting to     \n",
       "                             print graph.                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba3f750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task spec graph\n",
      "\n",
      "                                    \n",
      "                  DatasetLoading                    \n",
      "                                    \n",
      "                                                 \n",
      "                                                \n",
      "                                                  \n",
      "                               \n",
      " TokenizerTraining                                 \n",
      "                             \n",
      "                                              \n",
      "                                               \n",
      "                                                 \n",
      "                                 \n",
      "                              DatasetEncoding    \n",
      "                                \n",
      "                                                 \n",
      "                                                 \n",
      "                                                  \n",
      "                                          \n",
      "                     Training                       \n",
      "                                          \n",
      "                                                     \n",
      "                                                     \n",
      "                                                     \n",
      "                                    \n",
      "                  ModelSelection                    \n",
      "                                    \n",
      "                                                     \n",
      "                                                     \n",
      "                                                     \n",
      "                                        \n",
      "                    Evaluation                      \n",
      "                                        \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #800000\">WARNING </span> MainProcess  Console does not support paging. Defaulting to     \n",
       "                             print graph.                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfbaa8c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task graph\n",
      "\n",
      "                                                                                                                   \n",
      "                                                         DatasetLoading                                                          \n",
      "                                                                                                            \n",
      "                                                                                                                           \n",
      "                                                                                                                             \n",
      "                                                                                                                          \n",
      "                                                                                                                             \n",
      "                                                                                                                            \n",
      "                                                                                                            \n",
      "                                       TokenizerTraining                                                                      \n",
      "                                                                                                  \n",
      "                                                                                                           \n",
      "                                                                                                             \n",
      "                                                                                                              \n",
      "                                                                                                           \n",
      "                                                                                                                 \n",
      "                                                                                                        \n",
      "                                                                                                       DatasetEncoding   \n",
      "                                                                                          \n",
      "                                                                                                        \n",
      "                                                                                                       \n",
      "                                                                                                    \n",
      "                                                                                             \n",
      "                                                                                                          \n",
      "                                                                     \n",
      " Training-3            Training-4                                Training-1                Training-2                    \n",
      "                                                              \n",
      "                                                                                                                 \n",
      "                                                                                                                    \n",
      "                                                                                                                 \n",
      "                                                                                                                   \n",
      "                                                                                                                           \n",
      "                                                                                                                   \n",
      "                                                         ModelSelection                                                            \n",
      "                                                                                                                   \n",
      "                                                                                                                                    \n",
      "                                                                                                                                    \n",
      "                                                                                                                                    \n",
      "                                                                                                                                    \n",
      "                                                                                                                                    \n",
      "                                                                                                                       \n",
      "                                                           Evaluation                                                              \n",
      "                                                                                                                       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:53] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Download and save raw dataset to                   \n",
       "                             <span style=\"color: #008000\">\"DatasetLoading/000\"</span>.                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:54] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Finished task DatasetLoading <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">1</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">11</span>%<span style=\"font-weight: bold\">]</span>           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:57] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>    Save trained tokenizers to <span style=\"color: #008000\">\"TokenizerTraining/000\"</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f713d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>    Finished task TokenizerTraining <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">2</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">22</span>%<span style=\"font-weight: bold\">]</span>        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:58] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>    Save encoded dataset to <span style=\"color: #008000\">\"DatasetEncoding/000\"</span>.     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:45:59] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>    Finished task DatasetEncoding <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">3</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">33</span>%<span style=\"font-weight: bold\">]</span>          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:03] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>    Save model checkpoints to <span style=\"color: #008000\">\"Training/000/models\"</span>.   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>    Save model checkpoints to <span style=\"color: #008000\">\"Training/001/models\"</span>.   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:04] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>    Save model checkpoints to <span style=\"color: #008000\">\"Training/002/models\"</span>.   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Save model checkpoints to <span style=\"color: #008000\">\"Training/003/models\"</span>.   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba444d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:30] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">01</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">457316</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">4.289</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">72.905</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">3.330</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">27.945</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:31] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">01</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">735141</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">4.532</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">92.930</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">3.377</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">29.274</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:56] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">01</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:52</span>.<span style=\"color: #000080; font-weight: bold\">590331</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">4.027</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">56.110</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">3.303</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">27.198</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">02</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">150266</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.971</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">19.503</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.739</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">15.472</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">01</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:52</span>.<span style=\"color: #000080; font-weight: bold\">438514</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">4.095</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">60.010</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">3.128</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">22.834</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:46:58] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">02</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">926553</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.933</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">18.793</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.628</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">13.845</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:47:23] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">03</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">425008</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.321</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">10.191</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.329</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">10.271</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:47:25] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">03</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">743097</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.318</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">10.156</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.302</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">9.994</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:47:49] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">02</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:52</span>.<span style=\"color: #000080; font-weight: bold\">396688</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.711</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">15.051</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.468</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">11.798</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:47:50] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">02</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:53</span>.<span style=\"color: #000080; font-weight: bold\">184390</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">3.000</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">20.078</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.950</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">19.112</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">04</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">883701</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.878</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">6.543</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.189</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.930</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:47:52] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">04</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">817148</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.936</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">6.932</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.149</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.580</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba44750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:17] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">05</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">885408</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.574</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">4.826</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.122</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.346</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba3f3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:19] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">05</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">975297</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.658</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">5.248</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.071</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.933</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:42] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">03</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:53</span>.<span style=\"color: #000080; font-weight: bold\">301246</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.140</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">8.498</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.195</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.983</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:43] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">03</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:53</span>.<span style=\"color: #000080; font-weight: bold\">378186</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.537</span> | Train PPL:  <span style=\"color: #000080; font-weight: bold\">12.636</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.588</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">13.298</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:44] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">06</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">670690</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.338</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.811</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.135</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.461</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:48:46] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">06</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:27</span>.<span style=\"color: #000080; font-weight: bold\">068285</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.438</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">4.213</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.016</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.507</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:11] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">07</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">948091</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.148</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.153</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.149</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.575</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba3fd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:14] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">07</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:27</span>.<span style=\"color: #000080; font-weight: bold\">162851</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.252</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.497</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.008</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.450</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:36] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">04</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:53</span>.<span style=\"color: #000080; font-weight: bold\">741278</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.774</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">5.896</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.069</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.918</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:37] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">04</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:53</span>.<span style=\"color: #000080; font-weight: bold\">692577</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">2.115</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">8.288</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.384</span> |  Val. PPL:  <span style=\"color: #000080; font-weight: bold\">10.853</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:38] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">08</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:26</span>.<span style=\"color: #000080; font-weight: bold\">914509</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.000</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">2.717</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.190</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">8.934</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba73910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">4</span>    Finished task Training-<span style=\"color: #000080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">4</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">44</span>%<span style=\"font-weight: bold\">]</span>               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba73dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:49:41] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">08</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:27</span>.<span style=\"color: #000080; font-weight: bold\">012046</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.094</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">2.985</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.007</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.439</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90981610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">5</span>    Finished task Training-<span style=\"color: #000080; font-weight: bold\">2</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">5</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">56</span>%<span style=\"font-weight: bold\">]</span>               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfbaa8e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:50:15] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">05</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:38</span>.<span style=\"color: #000080; font-weight: bold\">539696</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.504</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">4.499</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.007</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.438</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfb9fb2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:50:25] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">05</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:47</span>.<span style=\"color: #000080; font-weight: bold\">607581</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.801</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">6.059</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.283</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">9.805</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba0a290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:50:53] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">06</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:38</span>.<span style=\"color: #000080; font-weight: bold\">158007</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.289</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.629</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">1.994</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.348</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba67cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:51:13] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">06</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:47</span>.<span style=\"color: #000080; font-weight: bold\">765087</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.565</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">4.783</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.282</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">9.793</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfb9fbe50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:51:31] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">07</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:37</span>.<span style=\"color: #000080; font-weight: bold\">602075</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.113</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.042</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.020</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.540</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfbaa8e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:52:00] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">07</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:46</span>.<span style=\"color: #000080; font-weight: bold\">569085</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.380</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.973</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.283</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">9.811</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfb9fb2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:52:09] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">08</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:37</span>.<span style=\"color: #000080; font-weight: bold\">778884</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">0.960</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">2.611</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.050</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">7.769</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba0a5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Finished task Training-<span style=\"color: #000080; font-weight: bold\">1</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">6</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">67</span>%<span style=\"font-weight: bold\">]</span>               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba73dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:52:47] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>                                                       \n",
       "                             Epoch: <span style=\"color: #000080; font-weight: bold\">08</span> | Time: <span style=\"color: #00ff00; font-weight: bold\">0:00:47</span>.<span style=\"color: #000080; font-weight: bold\">760005</span>                                \n",
       "                                     Train Loss: <span style=\"color: #000080; font-weight: bold\">1.229</span> | Train PPL:   <span style=\"color: #000080; font-weight: bold\">3.418</span>                  \n",
       "                                      Val. Loss: <span style=\"color: #000080; font-weight: bold\">2.293</span> |  Val. PPL:   <span style=\"color: #000080; font-weight: bold\">9.908</span>                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba73dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>    Finished task Training-<span style=\"color: #000080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">7</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">78</span>%<span style=\"font-weight: bold\">]</span>               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba01550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Save best run config to <span style=\"color: #008000\">\"ModelSelection/000\"</span>.      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba01e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">3</span>    Finished task ModelSelection <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">8</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">89</span>%<span style=\"font-weight: bold\">]</span>           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba73ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:52:51] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>    | Test Loss: <span style=\"color: #000080; font-weight: bold\">2.040</span> | Test PPL:   <span style=\"color: #000080; font-weight: bold\">7.691</span> |           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8c90f71890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dolphin-2 - Calculating BLEU: 100%|| 1000/1000 [00:57<00:00, 17.32sample/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[03/20/21 17:53:50] </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>    BLEU score = <span style=\"color: #000080; font-weight: bold\">35.18</span>                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba3f550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">                    </span><span style=\"color: #000080\">INFO    </span> Dolphin-<span style=\"color: #000080; font-weight: bold\">2</span>    Finished task Evaluation <span style=\"font-weight: bold\">[</span><span style=\"color: #000080; font-weight: bold\">9</span>/<span style=\"color: #000080; font-weight: bold\">9</span> - <span style=\"color: #000080; font-weight: bold\">100</span>%<span style=\"font-weight: bold\">]</span>              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8cfba12c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Swarm(n_dolphins=num_workers,\n",
    "           resources=resources,\n",
    "           results_store=results_store) as swarm:\n",
    "    flow = Flow(swarm=swarm)\n",
    "    flow.create(task_specs=tasks)\n",
    "\n",
    "    # visualize graphs\n",
    "    visualize_graph_in_console(flow.task_spec_graph, use_unicode=True)\n",
    "    visualize_graph_in_console(flow.task_graph, use_unicode=True)\n",
    "\n",
    "    flow.run(force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=\"400px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
