{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer based Sequence to Sequence Translation using FluidML\n",
    "In this notebook, we utilize FluidML to implement a complete ML pipeline that performs text translation from German to English.  \n",
    "Our translation pipeline consists of the following tasks:\n",
    "- **Dataset loading**: Downloads and parses the Multi30K dataset used for translation.\n",
    "- **Tokenizer training**: Trains and saves a Byte Pair Encoding (BPE) Tokenizer used for text encoding and decoding.\n",
    "- **Dataset encoding**: Encodes the dataset with the trained BPE Tokenizer.\n",
    "- **Model training**: Trains the Transformer based Sequence to Sequence Model.\n",
    "- **Model selection**: Selects from all trained model variations (different hyperparameter sweeps) the best performing one based on the validation set.\n",
    "- **Model evaluation**: Evaluates the best performing model on the test set by calculating the test loss and the bleu score.\n",
    "\n",
    "With FluidML, all of these steps are naturally implemented as individual tasks which register their dependencies and are chained together to a task graph. This graph is then executed in parallel by FluidML and all results are stored persistently in a local file store (see the Storage section for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To run this example it makes sense to install FluidML with the additional example requirements (Of course you can also manually install all dependencies. Check `transformer_seq2seq_translation.py` for a complete list). `cd` into the cloned fluidml directory and run:\n",
    "```bash \n",
    "pip install .[examples,rich-logging]\n",
    "```\n",
    "\n",
    "**Note**: Due to the limitation of multiprocessing and jupyter, we have to import our defined tasks and some helper classes from a separate script. Hence, our task definitions are located in `transformer_seq2seq_translation.py`, which not only implements the tasks but also the entire functionality of this example. So the interested reader can also go ahead and execute the just mentioned script. In order to still make this notebook self-explanatory, we provide Markdown code snippets of the individual task implementations at the place where we would have defined the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python internal imports\n",
    "import multiprocessing\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# External imports\n",
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# FluidML imports\n",
    "from fluidml import Flow, Swarm\n",
    "from fluidml.common import Task, Resource\n",
    "from fluidml.flow import GridTaskSpec, TaskSpec\n",
    "\n",
    "# Task imports, file store import and resource class import (see above note)\n",
    "from transformer_seq2seq_translation import DatasetLoading, TokenizerTraining, DatasetEncoding, Training, ModelSelection, Evaluation\n",
    "from transformer_seq2seq_translation import TaskResource, MyLocalFileStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2**: If you want to use FluidML's logging capability, please configure a logger using Python's `logging` API. For convenience, we provide a simple utility function which configures a visually appealing logger (using a specific handler from the `rich` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluidml.common.logging import configure_logging\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage - Saving Objects with FluidML\n",
    "\n",
    "Before we start with the implementation of our translation pipeline, we take a brief look at FluidML's storage API.  \n",
    "Out of the box FluidML provides three different storage options, which share the same interface (the interested user can implement his own storage, as long as the storage class inherits from `fluidml.storage.base.ResultsStore` and implements all abstract methods):\n",
    "- **InMemoryStore**: If no store object is provided, this is internally the default. Every saved object is stored in an in-memory manager dictionary, which is shared across all tasks and processes. Once the entire pipeline is executed, the results dictionary is returned to the user and it is the user's responsibility to actually save the results e.g. to disc. This store is only recommended for quick prototyping and small result objects, since intermediate task results cannot be stored persistently and the memory might not be sufficient to hold all task results.\n",
    "- **LocalFileStore**: A persistent file store implementation that out of the box supports saving files as .json and .pickle. It can be easily extended by the user to support arbitrary file types and save options.\n",
    "- **MongoDBStore**: A persistent MongoDBStore implementation, which stores saved objects as binary strings via GridFS in a Mongo DB.\n",
    "\n",
    "In this example we utilize the `LocalFileStore` and extend it with our own custom saving types.  \n",
    "In order to save an object within a task, one simply calls\n",
    "```python\n",
    "self.save(obj=model_state_dict, name='best_model', type_='torch')\n",
    "self.save(obj=some_dict, name='some_dict', type_='json')\n",
    "self.save(obj=some_serializable_obj, name='some_serializable_obj', type_='pickle')\n",
    "```\n",
    "Below, we implement `MyLocalFileStore`, which inherits from `LocalFileStore` and extends it by adding save and load functions for torch models and tokenizer objects.\n",
    "\n",
    "```python\n",
    "class MyLocalFileStore(LocalFileStore):\n",
    "    def __init__(self, base_dir: str):\n",
    "        super().__init__(base_dir=base_dir)\n",
    "\n",
    "        self._save_load_fn_from_type['torch'] = (self._save_torch, self._load_torch)\n",
    "        self._save_load_fn_from_type['tokenizer'] = (self._save_tokenizer, self._load_tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_torch(name: str, obj: Any, run_dir: str):\n",
    "        torch.save(obj, f=os.path.join(run_dir, f'{name}.pt'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_torch(name: str, run_dir: str) -> Any:\n",
    "        return torch.load(os.path.join(run_dir, f'{name}.pt'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_tokenizer(name: str, obj: Tokenizer, run_dir: str):\n",
    "        obj.save(os.path.join(run_dir, f'{name}.json'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_tokenizer(name: str, run_dir: str) -> Tokenizer:\n",
    "        return Tokenizer.from_file(os.path.join(run_dir, f'{name}.json'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Definitions\n",
    "\n",
    "The following 6 sections describe our task definitions in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Loading\n",
    "\n",
    "For this example we use the [Multi30K](https://github.com/multi30k/dataset) translation dataset, considering only German and English. The dataset was published with a fixed split of 29,000 train, 1,000 validation and 1,000 test German-English text pairs.\n",
    "\n",
    "We implement this Task by creating a custom `DatasetLoading` class, which inherits from FluidML's `Task` class. To comply with our interface a custom task just has to implement a `run()` method, which FluidML will execute internally.\n",
    "\n",
    "Here is our complete implementation of DatasetLoading (imported above):\n",
    "\n",
    "\n",
    "```python\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "\n",
    "class DatasetLoading(Task):\n",
    "    def __init__(self,\n",
    "                 base_url: str,\n",
    "                 data_split_names: Dict[str, List]):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.data_split_names = data_split_names\n",
    "\n",
    "    @staticmethod\n",
    "    def download_and_extract_gz_from_url(url: str) -> List[str]:\n",
    "        # download gz compressed data\n",
    "        data_gz = requests.get(url=url)\n",
    "        # decompress downloaded gz data to bytes object\n",
    "        data_bytes = gzip.decompress(data_gz.content)\n",
    "        # decode bytes object to utf-8 encoded str and convert to list by splitting on new line chars\n",
    "        data = data_bytes.decode('utf-8').splitlines()\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "        for split_name, files in self.data_split_names.items():\n",
    "            dataset = {}\n",
    "            for file_name in files:\n",
    "                url = self.base_url + file_name\n",
    "                language = file_name.split('.')[1]\n",
    "                data = DatasetLoading.download_and_extract_gz_from_url(url=url)\n",
    "                dataset[language] = data\n",
    "\n",
    "            self.save(obj=dataset, name=f'{split_name}_data', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizer Training\n",
    "\n",
    "We utilize Huggingface's [Tokenizers](https://github.com/huggingface/tokenizers) library to train a Byte Pair Encoding (BPE) tokenizer for the German and for the English sentences.\n",
    "\n",
    "\n",
    "```python\n",
    "from tokenizers.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class TokenizerTraining(Task):\n",
    "    def __init__(self, vocab_size: int, min_frequency: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "\n",
    "    def train_tokenizer(self, data: List[str]):\n",
    "        # initialize and train a tokenizer\n",
    "        tokenizer = CharBPETokenizer()\n",
    "        tokenizer.train_from_iterator(iterator=data,\n",
    "                                      vocab_size=self.vocab_size,\n",
    "                                      min_frequency=self.min_frequency,\n",
    "                                      special_tokens=['<unk>', '<bos>', '<eos>', '<pad>'],\n",
    "                                      show_progress=True)\n",
    "        \n",
    "        # add template rule to automatically add <bos> and <eos> to the encoding\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            pair=None,\n",
    "            special_tokens=[\n",
    "                (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "            ],\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "    def run(self, train_data: Dict[str, List[str]]):\n",
    "        # train german tokenizer\n",
    "        de_tokenizer = self.train_tokenizer(data=train_data['de'])\n",
    "\n",
    "        # train english tokenizer\n",
    "        en_tokenizer = self.train_tokenizer(data=train_data['en'])\n",
    "\n",
    "        # save tokenizers\n",
    "        self.save(obj=de_tokenizer, name='de_tokenizer', type_='tokenizer')\n",
    "        self.save(obj=en_tokenizer, name='en_tokenizer', type_='tokenizer')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataset Encoding\n",
    "\n",
    "We use the trained tokenizers for German and English to encode the previously saved datasets and save them as json files.  \n",
    "**Note**: FluidML automatically collects the required task inputs from the saved predecessor task results; in this case the datasets saved from `DatasetLoading` and the tokenizers trained in `TokenizerTraining`.\n",
    "\n",
    "\n",
    "```python\n",
    "class DatasetEncoding(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_data(data: Dict[str, List[str]],\n",
    "                    src_tokenizer: Tokenizer,\n",
    "                    trg_tokenizer: Tokenizer) -> List[Tuple[List[int], List[int]]]:\n",
    "\n",
    "        src_encoded = src_tokenizer.encode_batch(data['de'])\n",
    "        trg_encoded = trg_tokenizer.encode_batch(data['en'])\n",
    "        return [(src.ids, trg.ids) for src, trg in zip(src_encoded, trg_encoded)]\n",
    "\n",
    "    def run(self,\n",
    "            train_data: Dict[str, List[str]],\n",
    "            valid_data: Dict[str, List[str]],\n",
    "            test_data: Dict[str, List[str]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "\n",
    "        train_encoded = DatasetEncoding.encode_data(train_data, de_tokenizer, en_tokenizer)\n",
    "        valid_encoded = DatasetEncoding.encode_data(valid_data, de_tokenizer, en_tokenizer)\n",
    "        test_encoded = DatasetEncoding.encode_data(test_data, de_tokenizer, en_tokenizer)\n",
    "\n",
    "        self.save(obj=train_encoded, name='train_encoded', type_='json')\n",
    "        self.save(obj=valid_encoded, name='valid_encoded', type_='json')\n",
    "        self.save(obj=test_encoded, name='test_encoded', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training\n",
    "\n",
    "Our dataset is encoded using the trained BPE tokenizers, so the next step is to train the actual translation model. We utilize the well known transformer architecture first described in the [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper. Both, the encoder and decoder consist of several multi-head attention layers, which are the backbone of this architecture. Since this tutorial is about FluidML, we won't go into the implementational details of a transformer model. We refer the interested reader to [Ben Trevett's tutorial on transformers for translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb). Also, the pytorch transformer implementation used in this example is taken from Ben's tutorial.\n",
    "\n",
    "**Note**: The custom pytorch dataset, `TranslationDataset`, and the batch collate callable, `BatchCollator`, implementations can be also found in the above mentioned script, `transformer_seq2seq_translation.py`, from where the task classes are imported. Due to its complexity the transformer model implementation is also imported in this notebook and can be found in the `transformer_model.py` script. The definition of the `set_seed()` function is also located in `transformer_seq2seq_translation.py`.\n",
    "\n",
    "You also might note that we use `self.resource.seed` and `self.resource.device` in our training task without explicitly defining these in the `__init__()` method. When initializing FluidML's main classes `Flow` and `Swarm`, the user can optionally provide a list of resources that will be made available to all tasks and processes. In a machine learning context such resources could be but are not limited to seeds or cuda devices (see this example). Below we will go through how to define and provide the list of resources to `Flow` and how the resources will be automatically distributed to tasks.\n",
    "\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Training(Task):\n",
    "    def __init__(self,\n",
    "                 hid_dim: int,\n",
    "                 enc_layers: int,\n",
    "                 dec_layers: int,\n",
    "                 enc_heads: int,\n",
    "                 dec_heads: int,\n",
    "                 enc_pf_dim: int,\n",
    "                 dec_pf_dim: int,\n",
    "                 enc_dropout: float,\n",
    "                 dec_dropout: float,\n",
    "                 learning_rate: float,\n",
    "                 clip_grad: float,\n",
    "                 train_batch_size: int,\n",
    "                 valid_batch_size: int,\n",
    "                 num_epochs: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # transformer model parameters\n",
    "        self.hid_dim = hid_dim\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.enc_heads = enc_heads\n",
    "        self.dec_heads = dec_heads\n",
    "        self.enc_pf_dim = enc_pf_dim\n",
    "        self.dec_pf_dim = dec_pf_dim\n",
    "        self.enc_dropout = enc_dropout\n",
    "        self.dec_dropout = dec_dropout\n",
    "\n",
    "        # optimizer parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_grad = clip_grad\n",
    "\n",
    "        # dataloader and training loop parameters\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def _init_training(self, input_dim: int, output_dim: int, src_pad_idx: int, trg_pad_idx: int):\n",
    "        \"\"\" Initialize all training components.\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize the encoder and decoder block\n",
    "        enc = Encoder(input_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.enc_layers,\n",
    "                      self.enc_heads,\n",
    "                      self.enc_pf_dim,\n",
    "                      self.enc_dropout,\n",
    "                      self.resource.device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.dec_layers,\n",
    "                      self.dec_heads,\n",
    "                      self.dec_pf_dim,\n",
    "                      self.dec_dropout,\n",
    "                      self.resource.device)\n",
    "\n",
    "        # initialize the full transformer sequence to sequence model\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, self.resource.device).to(self.resource.device)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # initialize the loss criterion\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "        return model, optimizer, criterion\n",
    "\n",
    "    def _train_epoch(self, model, iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "            trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_epoch(model, iterator, criterion):\n",
    "        \"\"\" Validation loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, trg in iterator:\n",
    "\n",
    "                output, _ = model(src, trg[:, :-1])\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "                trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "                epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    def _train(self, model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop.\n",
    "        \"\"\"\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "\n",
    "            start_time = datetime.now()\n",
    "            train_loss = self._train_epoch(model, train_iterator, optimizer, criterion)\n",
    "            valid_loss = Training.validate_epoch(model, valid_iterator, criterion)\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            # if the current validation loss is below the previous best, update the best loss and\n",
    "            # save the new best model.\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model = model.state_dict()\n",
    "                self.save(obj=model.state_dict(), name='best_model', type_='torch')\n",
    "                self.save(obj={'epoch': epoch,\n",
    "                               'valid_loss': best_valid_loss}, name='best_model_metric', type_='json')\n",
    "\n",
    "            print(f'Epoch: {epoch + 1:02} | Time: {end_time - start_time}')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        assert best_model is not None\n",
    "        return best_model, best_valid_loss\n",
    "\n",
    "    def run(self,\n",
    "            train_encoded: List[Tuple[List[int], List[int]]],\n",
    "            valid_encoded: List[Tuple[List[int], List[int]]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "        set_seed(self.resource.seed)\n",
    "\n",
    "        # instantiate the collate fn for the dataloader\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "\n",
    "        # instantiate train and validation datasets using a pytorch's Dataset class\n",
    "        train_dataset = TranslationDataset(data=train_encoded)\n",
    "        valid_dataset = TranslationDataset(data=valid_encoded)\n",
    "\n",
    "        # instantiate train and validation dataloader\n",
    "        train_iterator = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True,\n",
    "                                    collate_fn=batch_collator)\n",
    "        valid_iterator = DataLoader(valid_dataset, batch_size=self.valid_batch_size, shuffle=False,\n",
    "                                    collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        # instantiate all training components\n",
    "        model, optimizer, criterion = self._init_training(input_dim=input_dim,\n",
    "                                                          output_dim=output_dim,\n",
    "                                                          src_pad_idx=src_pad_idx,\n",
    "                                                          trg_pad_idx=trg_pad_idx)\n",
    "\n",
    "        # train the model on the training set and evaluate after every epoch on the validation set\n",
    "        self._train(model=model,\n",
    "                    train_iterator=train_iterator,\n",
    "                    valid_iterator=valid_iterator,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Selection\n",
    "\n",
    "We have not yet talked about grid search and training several models with different hyperparameter combinations in parallel. FluidML provides a simple interface to allow just that, which we will describe further below when instantiating our tasks. For now let's assume that the previous tasks might have been executed multiple times with a set of different parameter combinations yielding several trained model variations.\n",
    "\n",
    "This task is a so called `reduce=True` task (will be used when instantiating the task), which means that it collects the results from all predecessor variations in order to compare and select the best performing variation. In this example it selects the best modelvariation based on the validation loss performance.\n",
    "\n",
    "\n",
    "```python\n",
    "class ModelSelection(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _select_best_model_from_sweeps(training_results: List[Dict]) -> Dict:\n",
    "        config = None\n",
    "        best_valid_loss = float('inf')\n",
    "        for sweep in training_results:\n",
    "            if sweep['result']['best_model_metric']['valid_loss'] <= best_valid_loss:\n",
    "                best_valid_loss = sweep['result']['best_model_metric']['valid_loss']\n",
    "                config = sweep['config']\n",
    "        return config\n",
    "\n",
    "    def run(self, reduced_results: List[Dict]):\n",
    "        \n",
    "        # select the best run config by comparing model performances from different parameter sweeps \n",
    "        # on the validation set\n",
    "        best_run_config = self._select_best_model_from_sweeps(training_results=reduced_results)\n",
    "\n",
    "        self.save(obj=best_run_config, name='best_run_config', type_='json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Evaluation\n",
    "\n",
    "This is the final task in our pipeline. It expects the previously determined `best_run_config` dictionary as input, loads the corresponding best model and tokenizers and evaluates said model on the test dataset.  \n",
    "First, we calculate the test set loss and perplexity. Second, we calculate the test set bleu score, since this is the standard metric of evaluating machine translation models.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "class Evaluation(Task):\n",
    "    def __init__(self, test_batch_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = test_batch_size\n",
    "\n",
    "    def _init_model(self, train_config: Dict, input_dim: int, output_dim: int,\n",
    "                    src_pad_idx: int, trg_pad_idx: int) -> nn.Module:\n",
    "        \"\"\" Initialize the model and its components.\n",
    "        \"\"\"\n",
    "        \n",
    "        enc = Encoder(input_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['enc_layers'],\n",
    "                      train_config['enc_heads'],\n",
    "                      train_config['enc_pf_dim'],\n",
    "                      train_config['enc_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['dec_layers'],\n",
    "                      train_config['dec_heads'],\n",
    "                      train_config['dec_pf_dim'],\n",
    "                      train_config['dec_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, self.resource.device).to(self.resource.device)\n",
    "        return model\n",
    "\n",
    "    def translate_sentence(self, src_encoded, bos_idx, eos_idx, model, max_len=50):\n",
    "        \"\"\" Translate an encoded sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        src_tensor = torch.LongTensor(src_encoded).unsqueeze(0).to(self.resource.device)\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "        trg_indices = [bos_idx]\n",
    "        for i in range(max_len):\n",
    "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(self.resource.device)\n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "            with torch.no_grad():\n",
    "                output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == eos_idx:\n",
    "                break\n",
    "\n",
    "        return trg_indices[1:]\n",
    "\n",
    "    def calculate_bleu(self, data_encoded, en_tokenizer, model, max_len=50):\n",
    "        \"\"\" Calculate the bleu score on the test set.\n",
    "        \"\"\"\n",
    "        \n",
    "        trgs = []\n",
    "        pred_trgs = []\n",
    "        bos_idx = en_tokenizer.token_to_id('<bos>')\n",
    "        eos_idx = en_tokenizer.token_to_id('<eos>')\n",
    "\n",
    "        for src, trg in tqdm(data_encoded):\n",
    "\n",
    "            pred_trg = self.translate_sentence(src, bos_idx, eos_idx, model, max_len)\n",
    "\n",
    "            # cut off <eos> token\n",
    "            pred_trg = pred_trg[:-1]\n",
    "\n",
    "            pred_trg_decoded = en_tokenizer.decode(pred_trg)\n",
    "            pred_trgs.append(pred_trg_decoded.split())\n",
    "\n",
    "            trg_decoded = en_tokenizer.decode(trg)\n",
    "            trgs.append(trg_decoded.split())\n",
    "\n",
    "        return bleu_score(pred_trgs, trgs)\n",
    "\n",
    "    def run(self, best_run_config: Dict):\n",
    "        set_seed(self.resource.seed)\n",
    "\n",
    "        # load the best model, test-data and the tokenizers based on the previously selected best run config\n",
    "        model_state_dict = self.load(name='best_model', task_name='Training', task_unique_config=best_run_config)\n",
    "        test_encoded = self.load(name='test_encoded', task_name='DatasetEncoding', task_unique_config=best_run_config)\n",
    "        de_tokenizer = self.load(name='de_tokenizer', task_name='TokenizerTraining', task_unique_config=best_run_config)\n",
    "        en_tokenizer = self.load(name='en_tokenizer', task_name='TokenizerTraining', task_unique_config=best_run_config)\n",
    "\n",
    "        # instantiate the batch collator\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "        \n",
    "        # instantiate the test dataset\n",
    "        test_dataset = TranslationDataset(data=test_encoded)\n",
    "\n",
    "        # instantiate the test dataloader\n",
    "        test_iterator = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        # instantiate the transformer model\n",
    "        model = self._init_model(train_config=best_run_config['Training'],\n",
    "                                 input_dim=input_dim,\n",
    "                                 output_dim=output_dim,\n",
    "                                 src_pad_idx=src_pad_idx,\n",
    "                                 trg_pad_idx=src_pad_idx)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "\n",
    "        # instantiate the loss criterion\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "        # evaluate the model on the test set -> calculate the test set loss and perplexity\n",
    "        test_loss = Training.validate_epoch(model=model, iterator=test_iterator, criterion=criterion)\n",
    "        print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "        # calculate the model's bleu score on the test set\n",
    "        bleu = self.calculate_bleu(test_encoded, en_tokenizer, model)\n",
    "        print(f'BLEU score = {bleu * 100:.2f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Pipeline/Task-Graph via FluidML\n",
    "\n",
    "So far, we have looked into implementing our individual pipeline steps using FluidML's Task class and it was very straightforward.\n",
    "You might be wondering, how to put these tasks together and make them work together as a single pipeline?\n",
    "\n",
    "Thanks to FluidML's TaskSpec API, you can connect these tasks like Lego blocks :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiate Task Specs\n",
    "`TaskSpec` and `GridTaskSpec` are simple wrapper classes that allow to specify task details and task arguments which will be used during instantiation of the task.\n",
    "Let's go ahead and create specs for all our tasks.  \n",
    "\n",
    "**Note 1**: If a task expects specific input objects and publishes result objects, that are relevent to successor tasks, these object names have to be registered through the `expects` and `publishes` attributes. This is necessary because of two reasons. First, FluidML internally uses this information to automatically collect all required input objects from the result store (e.g. `LocalFileStore`) and provide them to the task through the `run(*collected_inputs)` method. Second, based on this information FluidML decides whether a task still needs to be executed or whether it has been executed already in a previous run and all results can be loaded from the store.\n",
    "\n",
    "**Note 2**: A task where no hyperparameter tuning or grid search is necessary is wrapped in the `TaskSpec` class. Likewise, tasks, where we want to perform hyperparameter tuning, e.g. `TokenizerTraining` and `Training`, are wrapped in the dedicated `GridTaskSpec` class.\n",
    "\n",
    "Below we define for each task the necessary parameter dictionary which we feed into the task spec class.  \n",
    "\n",
    "**Note 3**: All parameters of a `GridTaskSpec` task that are stored in a list, will be automatically expanded by `Flow` to create different task instances for each explicit parameter combination. For example, considering the `Training` task, internally flow will instantiate 4 train tasks with the cross product combinations of different `train_batch_size` and `learning_rate`.\n",
    "\n",
    "**Note 4**: If you have to provide a parameter to a `GridTaskSpec` task, which is of type `List` so it should not get expanded, you have to wrap it again in a second list. E.g. `layer_dimensions: [[64, 128, 64]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "\n",
    "dataset_loading_params = {'base_url': 'https://raw.githubusercontent.com/multi30k/dataset/'\n",
    "                                      'master/data/task1/raw/',\n",
    "                          'data_split_names': {'train': ['train.de.gz', 'train.en.gz'],\n",
    "                                               'valid': ['val.de.gz', 'val.en.gz'],\n",
    "                                               'test': ['test_2016_flickr.de.gz', 'test_2016_flickr.en.gz']}}\n",
    "dataset_loading_task = TaskSpec(task=DatasetLoading, task_kwargs=dataset_loading_params,\n",
    "                                publishes=['train_data', 'valid_data', 'test_data'])\n",
    "\n",
    "\n",
    "tokenizer_training_params = {'vocab_size': 30000,\n",
    "                             'min_frequency': 2}\n",
    "tokenizer_training_task = GridTaskSpec(task=TokenizerTraining, gs_config=tokenizer_training_params,\n",
    "                                       expects=['train_data'],\n",
    "                                       publishes=['de_tokenizer', 'en_tokenizer'])\n",
    "\n",
    "\n",
    "dataset_encoding_task = TaskSpec(task=DatasetEncoding,\n",
    "                                 expects=['train_data', 'valid_data', 'test_data', 'de_tokenizer', 'en_tokenizer'],\n",
    "                                 publishes=['train_encoded', 'valid_encoded', 'test_encoded'])\n",
    "\n",
    "\n",
    "training_params = {'hid_dim': 256,\n",
    "                   'enc_layers': 3,\n",
    "                   'dec_layers': 3,\n",
    "                   'enc_heads': 8,\n",
    "                   'dec_heads': 8,\n",
    "                   'enc_pf_dim': 512,\n",
    "                   'dec_pf_dim': 512,\n",
    "                   'enc_dropout': 0.1,\n",
    "                   'dec_dropout': 0.1,\n",
    "                   'learning_rate': [0.0005, 0.001],\n",
    "                   'clip_grad': 1.,\n",
    "                   'train_batch_size': [64, 128],\n",
    "                   'valid_batch_size': 64,\n",
    "                   'num_epochs': 10}\n",
    "train_task = GridTaskSpec(task=Training, gs_config=training_params,\n",
    "                          expects=['train_encoded', 'valid_encoded', 'de_tokenizer', 'en_tokenizer'],\n",
    "                          publishes=['best_model', 'best_model_metric'])\n",
    "\n",
    "\n",
    "model_selection_task = TaskSpec(task=ModelSelection, reduce=True, expects=['best_model_metric'],\n",
    "                                publishes=['best_run_config'])\n",
    "\n",
    "\n",
    "evaluation_params = {'test_batch_size': 128}\n",
    "evaluate_task = TaskSpec(task=Evaluation, reduce=False, task_kwargs=evaluation_params,\n",
    "                         expects=['best_run_config'], publishes=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Registering all Task Dependencies\n",
    "After having instantiated all task specs, we utilize the `requires()` method to register dependencies between tasks.\n",
    "\n",
    "Using these task dependencies, FluidML's `Flow` class properly expands all `GridTaskSpecs` and creates a task graph. Next, FluidML's `Swarm` class schedules and performs the parallel task executions considering the registered dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dependencies between tasks\n",
    "\n",
    "tokenizer_training_task.requires([dataset_loading_task])\n",
    "dataset_encoding_task.requires([tokenizer_training_task, dataset_loading_task])\n",
    "train_task.requires([dataset_encoding_task, tokenizer_training_task])\n",
    "model_selection_task.requires([train_task])\n",
    "evaluate_task.requires([model_selection_task])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating the List of Task Spec Instances\n",
    "We pack all these task specs in a list which gets passed to FluidML. `Flow` internally creates the task instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [dataset_loading_task, tokenizer_training_task, dataset_encoding_task, train_task, model_selection_task, evaluate_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setting all Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamically get the directory of this script\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "# define the base directory where all our task results will be stored in a structured way using LocalFileStore\n",
    "base_dir = os.path.join(current_dir, 'seq2seq_experiments')\n",
    "\n",
    "# select the number of workers (processes used to execute tasks in parallel)\n",
    "num_workers = 2\n",
    "\n",
    "# set force to 'selected' if you want to force-execute the selected task (task_to_execute has to be set, e.g. task_to_execute='Training') \n",
    "#   regardless previously task results being available. \n",
    "# set force to 'all' if all tasks in the pipeline have to be force-executed.\n",
    "# set force to None if already existing tasks are skipped so that results cen be loaded from the store.\n",
    "force = None  # choices [selected, all, None]\n",
    "\n",
    "# Only run the pipeline up to the selected task (including the task). All successor tasks defined in the pipeline will not be executed.\n",
    "task_to_execute = None\n",
    "\n",
    "# try to use cuda GPU's if available\n",
    "use_cuda = True\n",
    "\n",
    "# set a global seed for the entire pipeline\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define and instantiate Resources to share across all Tasks\n",
    "\n",
    "We mentioned already during the `Training` task that FluidML enables the user to conveniently share resources across all tasks instead of providing them explicitely to each task individually.  \n",
    "The user achieves this by creating his own Resource dataclass, which inherits from our `Resource` interface. In this dataclass we define all resources, in our case the seed, and the cuda device, which we make available to all tasks through the `self.resource` attribute.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TaskResource(Resource):\n",
    "    device: str\n",
    "    seed: int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we utilize a little helper function to distribute our available cuda devices euqally across the number of defined workers.  \n",
    "E.g. let's assume we selected `num_workers = 4` and we have access to two GPU's, the below function would return the following balanced list of devices:\n",
    "\n",
    "```python\n",
    "print(devices)\n",
    "-> ['cuda:0', 'cuda:1', 'cuda:0', 'cuda:1']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_devices(count: Optional[int] = None,\n",
    "                         use_cuda: bool = True) -> List[str]:\n",
    "    count = count if count is not None else multiprocessing.cpu_count()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        devices = [f'cuda:{id_}' for id_ in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        devices = ['cpu']\n",
    "    factor = int(count / len(devices))\n",
    "    remainder = count % len(devices)\n",
    "    devices = devices * factor + devices[:remainder]\n",
    "    return devices\n",
    "\n",
    "devices = get_balanced_devices(count=num_workers, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our list of resource objects which we will feed to the `Swarm` class during instantiation.  \n",
    "**Note**: `len(resources) == num_workers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of resources\n",
    "resources = [TaskResource(device=devices[i], seed=seed) for i in range(num_workers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Instantiate the previously defined File Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local file storage used for versioning\n",
    "results_store = MyLocalFileStore(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Run the Pipeline/Task-Graph\n",
    "\n",
    "We use a context manager to instantiate the `Swarm` class, pass the swarm instance to the `Flow` class and finally run the pipeline.  \n",
    "All saved task results will be availabe in the previously selected output directory (if not changed: `seq2seq_experiments`).  \n",
    "Feel free to play around with different hyperparameter combinations to improve your model's translation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Swarm(n_dolphins=num_workers,\n",
    "           resources=resources,\n",
    "           results_store=results_store\n",
    "           refresh_every=1) as swarm:\n",
    "    flow = Flow(swarm=swarm, task_to_execute=task_to_execute, force=force)\n",
    "    flow.run(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=400 height=400 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
