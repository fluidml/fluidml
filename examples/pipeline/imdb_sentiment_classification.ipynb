{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "from datasets import load_dataset\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from examples.utils.gpu import get_balanced_devices\n",
    "from fluidml.common import Resource\n",
    "from fluidml.flow import Flow, GridTaskSpec, TaskSpec\n",
    "from fluidml.swarm import Swarm\n",
    "from fluidml.swarm.storage import LocalFileStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFetcher(Task):\n",
    "    def __init__(self, name: str, id_: int, fetch_param: int):\n",
    "        super().__init__(name=name, id_=id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for item in dataset[\"train\"]:\n",
    "            if len(item[\"topics\"]) > 0:\n",
    "                sentences.append(item[\"text\"])\n",
    "                labels.append(item[\"topics\"][0])\n",
    "        task_results = {\"sentences\": sentences,\n",
    "                        \"labels\": labels}\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(Task):\n",
    "    def __init__(self, name: str, id_: int, fetch_param: int):\n",
    "        super().__init__(name=name, id_=id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for item in dataset[\"train\"]:\n",
    "            if len(item[\"topics\"]) > 0:\n",
    "                sentences.append(item[\"text\"])\n",
    "                labels.append(item[\"topics\"][0])\n",
    "        task_results = {\"sentences\": sentences,\n",
    "                        \"labels\": labels}\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfFeaturizer(Task):\n",
    "    def __init__(self, name: str, id_: int, tfidf_param: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        tfidf = TfidfVectorizer()\n",
    "        tfidf_vectors = tfidf.fit_transform(results[\"pre_process\"][\"sentences\"]).toarray()\n",
    "        task_results = {\n",
    "            \"vectors\": tfidf_vectors\n",
    "        }\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveFeaturizer(Task):\n",
    "    def __init__(self, name: str, id_: int, glove_param: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        sentences = [Sentence(sent) for sent in results[\"pre_process\"][\"sentences\"]]\n",
    "        embedder = DocumentPoolEmbeddings([WordEmbeddings(\"glove\")])\n",
    "        embedder.embed(sentences)\n",
    "        glove_vectors = [sent.embedding.cpu().numpy() for sent in sentences]\n",
    "        glove_vectors = np.array(glove_vectors).reshape(len(glove_vectors), -1)\n",
    "        task_results = {\n",
    "            \"vectors\": glove_vectors\n",
    "        }\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Task):\n",
    "    def __init__(self, name: str, id_: int, train_param: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        model = LogisticRegression(max_iter=50)\n",
    "        stacked_vectors = np.hstack((results[\"tfidf_featurize\"][\"vectors\"], results[\"glove_featurize\"][\"vectors\"]))\n",
    "        model.fit(stacked_vectors, results[\"dataset\"][\"labels\"])\n",
    "        task_results = {\n",
    "            \"model\": model,\n",
    "            \"vectors\": stacked_vectors,\n",
    "            \"labels\": results[\"dataset\"][\"labels\"],\n",
    "        }\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluater(Task):\n",
    "    def __init__(self, name: str, id_: int, eval_param: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        predictions = results[\"train\"][\"model\"].predict(results[\"train\"][\"vectors\"])\n",
    "        report = classification_report(results[\"train\"][\"labels\"], predictions)\n",
    "        task_results = {\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "        return task_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "fetch = TaskSpec(task=DatasetFetcher, name=\"fetch\", task_kwargs={\"fetch_param\": 1})\n",
    "preprocess = GridTaskSpec(task=Preprocessor, name=\"preprocess\", gs_config={\"process_param\": 1})\n",
    "featurize_glove = GridTaskSpec(task=GloveFeaturizer, name=\"featurize_glove\", gs_config={\"glove_param\": [5, 10]})\n",
    "featurize_tfidf = GridTaskSpec(task=TfIdfFeaturizer, name=\"featurize_tfidf\", gs_config={\"tfidf_param\": 10})\n",
    "train = GridTaskSpec(task=Trainer, name=\"train\", gs_config={\"train_param\": 10})\n",
    "evaluate = GridTaskSpec(task=Evaluater, name=\"evaluate\", gs_config={\"eval_param\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register dependencies between tasks\n",
    "preprocess.requires([fetch])\n",
    "featurize_glove.requires([preprocess])\n",
    "featurize_tfidf.requires([preprocess])\n",
    "train.requires([fetch, featurize_glove, featurize_tfidf])\n",
    "evaluate.requires([train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [fetch,\n",
    "         preprocess,\n",
    "         featurize_glove, featurize_tfidf,\n",
    "         train,\n",
    "         evaluate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Swarm(n_dolphins=3, refresh_every=5) as swarm:\n",
    "    flow = Flow(swarm=swarm)\n",
    "    results = flow.run(tasks)\n",
    "print(results[\"evaluate\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
