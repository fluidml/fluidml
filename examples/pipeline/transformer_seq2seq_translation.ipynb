{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer based Sequence to Sequence Translation using FluidML\n",
    "In this notebook, we utilize FluidML to implement a complete ML pipeline that performs text translation from German to English.  \n",
    "Our translation pipeline consists of the following tasks:\n",
    "- **Dataset loading**: Downloads and parses the Multi30K dataset used for translation.\n",
    "- **Tokenizer training**: Trains and saves a Byte Pair Encoding (BPE) Tokenizer used for text encoding and decoding.\n",
    "- **Dataset encoding**: Encodes the dataset with the trained BPE Tokenizer.\n",
    "- **Model Training**: Trains the Transformer based Sequence to Sequence Model.\n",
    "- **Model Selection and Evaluation**: Selects from all trained model variations the best performing one and evaluates it on the test set.\n",
    "\n",
    "With FluidML, all of these steps are naturally implemented as individual tasks which register their dependencies and are chained together to a task graph. This graph is then executed in parallel by FluidML and all results are stored persistently in a local file store (see the Storage section for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Note**: Due to the limitation of multiprocessing and jupyter, we have to import our defined tasks from a separate script. In order to still make this notebook self-explanatory, we provide Markdown code snippets of the individual task implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python internal imports\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import requests\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# FluidML imports\n",
    "from fluidml import Flow, Swarm\n",
    "from fluidml.common import Task, Resource\n",
    "from fluidml.flow import GridTaskSpec, TaskSpec\n",
    "from fluidml.storage import LocalFileStore\n",
    "\n",
    "# Task imports (see above note)\n",
    "from transformer_seq2seq_translation import DatasetLoading, TokenizerTraining, DatasetEncoding, Training, Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage - Saving Objects with FluidML\n",
    "\n",
    "Before we start with the implementation of our translation pipeline, we take a brief look at FluidML's storage API.  \n",
    "Out of the box FluidML provides three different storage options, which share the same interface (the interested user can implement his own storage, as long as the storage class inherits from `fluidml.storage.base.ResultsStore` and implements all abstract methods):\n",
    "- **InMemoryStore**: If no store object is provided, this is internally the default. Every saved object is stored in an in-memory manager dictionary, which is shared across all tasks and processes. Once the entire pipeline is executed, the results dictionary is returned to the user and it is the user's responsibility to actually save the results e.g. to disc. This store is only recommended for quick prototyping and small result objects, since intermediate task results cannot be stored persistently and the memory might not be sufficient to hold all task results.\n",
    "- **LocalFileStore**: A persistent file store implementation that out of the box supports saving files as .json and .pickle. It can be easily extended by the user to support arbitrary file types and save options.\n",
    "- **MongoDBStore**: A persistent MongoDBStore implementation, which stores saved objects as binary strings via GridFS in a Mongo DB.\n",
    "\n",
    "In this example we utilize the `LocalFileStore` and extend it with our own custom saving types.  \n",
    "In order to save an object within a task, one simply calls\n",
    "```python\n",
    "self.save(obj=model_state_dict, name='best_model', type_='torch')\n",
    "self.save(obj=some_dict, name='some_dict', type_='json')\n",
    "self.save(obj=some_serializable_obj, name='some_serializable_obj', type_='pickle')\n",
    "```\n",
    "Below, we implement `MyLocalFileStore`, which inherits from `LocalFileStore` and extends it by adding save and load functions for torch models and tokenizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLocalFileStore(LocalFileStore):\n",
    "    def __init__(self, base_dir: str):\n",
    "        super().__init__(base_dir=base_dir)\n",
    "\n",
    "        self._save_load_fn_from_type['torch'] = (self._save_torch, self._load_torch)\n",
    "        self._save_load_fn_from_type['tokenizer'] = (self._save_tokenizer, self._load_tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_torch(name: str, obj: Any, run_dir: str):\n",
    "        torch.save(obj, f=os.path.join(run_dir, f'{name}.pt'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_torch(name: str, run_dir: str) -> Any:\n",
    "        return torch.load(os.path.join(run_dir, f'{name}.pt'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_tokenizer(name: str, obj: Tokenizer, run_dir: str):\n",
    "        obj.save(os.path.join(run_dir, f'{name}.json'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_tokenizer(name: str, run_dir: str) -> Tokenizer:\n",
    "        return Tokenizer.from_file(os.path.join(run_dir, f'{name}.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading\n",
    "\n",
    "For this example we use the [Multi30K](https://github.com/multi30k/dataset) translation dataset, considering only German and English. The dataset was published with a fixed split of 29,000 train, 1,000 validation and 1,000 test German-English text pairs.\n",
    "\n",
    "We implement this Task by creating a custom `DatasetLoading` class, which inherits from FluidML's `Task` class. To comply with our interface a custom task just has to implement a `run()` method, which FluidML will execute internally.\n",
    "\n",
    "Here is our complete implementation of DatasetLoading (imported above):\n",
    "\n",
    "\n",
    "```python\n",
    "import gzip\n",
    "import requests\n",
    "\n",
    "\n",
    "class DatasetLoading(Task):\n",
    "    def __init__(self,\n",
    "                 base_url: str,\n",
    "                 data_split_names: Dict[str, List]):\n",
    "        super().__init__()\n",
    "        self.base_url = base_url\n",
    "        self.data_split_names = data_split_names\n",
    "\n",
    "    @staticmethod\n",
    "    def download_and_extract_gz_from_url(url: str) -> List[str]:\n",
    "        # download gz compressed data\n",
    "        data_gz = requests.get(url=url)\n",
    "        # decompress downloaded gz data to bytes object\n",
    "        data_bytes = gzip.decompress(data_gz.content)\n",
    "        # decode bytes object to utf-8 encoded str and convert to list by splitting on new line chars\n",
    "        data = data_bytes.decode('utf-8').splitlines()\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "        for split_name, files in self.data_split_names.items():\n",
    "            dataset = {}\n",
    "            for file_name in files:\n",
    "                url = self.base_url + file_name\n",
    "                language = file_name.split('.')[1]\n",
    "                data = DatasetLoading.download_and_extract_gz_from_url(url=url)\n",
    "                dataset[language] = data\n",
    "\n",
    "            self.save(obj=dataset, name=f'{split_name}_data', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizer Training\n",
    "\n",
    "We utilize Huggingface's [Tokenizers](https://github.com/huggingface/tokenizers) library to train a Bayte Pair Encoding (BPE) tokenizer for the German and for the English sentences.\n",
    "\n",
    "\n",
    "```python\n",
    "from tokenizrs.implementations import CharBPETokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class TokenizerTraining(Task):\n",
    "    def __init__(self, vocab_size: int, min_frequency: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "\n",
    "    def train_tokenizer(self, data: List[str]):\n",
    "        # initialize and train a tokenizer\n",
    "        tokenizer = CharBPETokenizer()\n",
    "        tokenizer.train_from_iterator(iterator=data,\n",
    "                                      vocab_size=self.vocab_size,\n",
    "                                      min_frequency=self.min_frequency,\n",
    "                                      special_tokens=['<unk>', '<bos>', '<eos>', '<pad>'],\n",
    "                                      show_progress=True)\n",
    "        \n",
    "        # add template rule to automatically add <bos> and <eos> to the encoding\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"<bos> $A <eos>\",\n",
    "            pair=None,\n",
    "            special_tokens=[\n",
    "                (\"<bos>\", tokenizer.token_to_id(\"<bos>\")),\n",
    "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
    "            ],\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "    def run(self, train_data: Dict[str, List[str]]):\n",
    "        # train german tokenizer\n",
    "        de_tokenizer = self.train_tokenizer(data=train_data['de'])\n",
    "\n",
    "        # train english tokenizer\n",
    "        en_tokenizer = self.train_tokenizer(data=train_data['en'])\n",
    "\n",
    "        # save tokenizers\n",
    "        self.save(obj=de_tokenizer, name='de_tokenizer', type_='tokenizer')\n",
    "        self.save(obj=en_tokenizer, name='en_tokenizer', type_='tokenizer')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Encoding\n",
    "\n",
    "We use the trained tokenizers for German and English to encode the previously saved datasets and save them as json files.  \n",
    "**Note**: FluidML automatically collects the required task inputs from the saved predecessor task results, in this case the datasets saved from `DatasetLoading` and the tokenizers trained in `TokenizerTraining`.\n",
    "\n",
    "\n",
    "```python\n",
    "class DatasetEncoding(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_data(data: Dict[str, List[str]],\n",
    "                    src_tokenizer: Tokenizer,\n",
    "                    trg_tokenizer: Tokenizer) -> List[Tuple[List[int], List[int]]]:\n",
    "\n",
    "        src_encoded = src_tokenizer.encode_batch(data['de'])\n",
    "        trg_encoded = trg_tokenizer.encode_batch(data['en'])\n",
    "        return [(src.ids, trg.ids) for src, trg in zip(src_encoded, trg_encoded)]\n",
    "\n",
    "    def run(self,\n",
    "            train_data: Dict[str, List[str]],\n",
    "            valid_data: Dict[str, List[str]],\n",
    "            test_data: Dict[str, List[str]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "\n",
    "        train_encoded = DatasetEncoding.encode_data(train_data, de_tokenizer, en_tokenizer)\n",
    "        valid_encoded = DatasetEncoding.encode_data(valid_data, de_tokenizer, en_tokenizer)\n",
    "        test_encoded = DatasetEncoding.encode_data(test_data, de_tokenizer, en_tokenizer)\n",
    "\n",
    "        self.save(obj=train_encoded, name='train_encoded', type_='json')\n",
    "        self.save(obj=valid_encoded, name='valid_encoded', type_='json')\n",
    "        self.save(obj=test_encoded, name='test_encoded', type_='json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Our dataset is encoded using the trained BPE tokenizers, so the next step is to train the actual translation model. We utilize the well known transformer architecture first described in the [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper. Both, the encoder and decoder consist of several multi-head attention layers, which are the backbone of this architecture. Since this tutorial is about FluidML, we won't go into the implementational details of a transformer model. We refer the interested reader to [Ben Trevett's tutorial on transformers for translation](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb). Also, the pytorch transformer implementation used in this example is taken from Ben's tutorial.\n",
    "\n",
    "**Note**: The custom pytorch dataset and collate function implementations can be found in the same script, `transformer_seq2seq_translation.py`, from where the task classes are imported. The Transformer model implementation is also imported in this notebook ans can be found in the `transformer_model.py` script.\n",
    "\n",
    "\n",
    "```python\n",
    "class Training(Task):\n",
    "    def __init__(self,\n",
    "                 hid_dim: int,\n",
    "                 enc_layers: int,\n",
    "                 dec_layers: int,\n",
    "                 enc_heads: int,\n",
    "                 dec_heads: int,\n",
    "                 enc_pf_dim: int,\n",
    "                 dec_pf_dim: int,\n",
    "                 enc_dropout: float,\n",
    "                 dec_dropout: float,\n",
    "                 learning_rate: float,\n",
    "                 clip_grad: float,\n",
    "                 train_batch_size: int,\n",
    "                 valid_batch_size: int,\n",
    "                 num_epochs: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # transformer model parameters\n",
    "        self.hid_dim = hid_dim\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        self.enc_heads = enc_heads\n",
    "        self.dec_heads = dec_heads\n",
    "        self.enc_pf_dim = enc_pf_dim\n",
    "        self.dec_pf_dim = dec_pf_dim\n",
    "        self.enc_dropout = enc_dropout\n",
    "        self.dec_dropout = dec_dropout\n",
    "        \n",
    "        # optimizer parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_grad = clip_grad\n",
    "\n",
    "        # dataloader and training loop parameters\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.valid_batch_size = valid_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def _init_training(self, input_dim: int, output_dim: int, src_pad_idx: int, trg_pad_idx: int, device: torch.device):\n",
    "        \"\"\" Initialize all training components.\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize the encoder and decoder block\n",
    "        enc = Encoder(input_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.enc_layers,\n",
    "                      self.enc_heads,\n",
    "                      self.enc_pf_dim,\n",
    "                      self.enc_dropout,\n",
    "                      device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      self.hid_dim,\n",
    "                      self.dec_layers,\n",
    "                      self.dec_heads,\n",
    "                      self.dec_pf_dim,\n",
    "                      self.dec_dropout,\n",
    "                      device)\n",
    "\n",
    "        # initialize the full transformer sequence to sequence model\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, device).to(device)\n",
    "\n",
    "        # initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # initialize the loss criterion\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "        return model, optimizer, criterion\n",
    "\n",
    "    def _train_epoch(self, model, iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "            trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), self.clip_grad)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_epoch(model, iterator, criterion):\n",
    "        \"\"\" Validation loop to iterate over batches\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, trg in iterator:\n",
    "\n",
    "                output, _ = model(src, trg[:, :-1])\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output.contiguous().view(-1, output_dim)  # [batch size * trg len - 1, output dim]\n",
    "                trg = trg[:, 1:].contiguous().view(-1)             # [batch size * trg len - 1]\n",
    "\n",
    "                loss = criterion(output, trg)\n",
    "                epoch_loss += loss.item()\n",
    "        return epoch_loss / len(iterator)\n",
    "\n",
    "    def _train(self, model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "        \"\"\" Train loop.\n",
    "        \"\"\"\n",
    "        \n",
    "        best_valid_loss = float('inf')\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "\n",
    "            start_time = datetime.now()\n",
    "            train_loss = self._train_epoch(model, train_iterator, optimizer, criterion)\n",
    "            valid_loss = Training.validate_epoch(model, valid_iterator, criterion)\n",
    "            end_time = datetime.now()\n",
    "\n",
    "            # if the current validation loss is below the previous best, update the best loss and \n",
    "            # save the new best model.\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model = model.state_dict()\n",
    "                self.save(obj=model.state_dict(), name='best_model', type_='torch')\n",
    "                self.save(obj={'epoch': epoch,\n",
    "                               'valid_loss': best_valid_loss}, name='best_model_metric', type_='json')\n",
    "\n",
    "            print(f'Epoch: {epoch + 1:02} | Time: {end_time - start_time}')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        assert best_model is not None\n",
    "        return best_model, best_valid_loss\n",
    "\n",
    "    def run(self,\n",
    "            train_encoded: List[Tuple[List[int], List[int]]],\n",
    "            valid_encoded: List[Tuple[List[int], List[int]]],\n",
    "            de_tokenizer: Tokenizer,\n",
    "            en_tokenizer: Tokenizer):\n",
    "        set_seed(self.resource.seed)\n",
    "\n",
    "        # instantiate the collate fn for the dataloader\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "\n",
    "        # instantiate train and validation datasets using a pytorch's Dataset class\n",
    "        train_dataset = TranslationDataset(data=train_encoded[:100])\n",
    "        valid_dataset = TranslationDataset(data=valid_encoded[:100])\n",
    "\n",
    "        # instantiate train and validation dataloader\n",
    "        train_iterator = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=True,\n",
    "                                    collate_fn=batch_collator)\n",
    "        valid_iterator = DataLoader(valid_dataset, batch_size=self.valid_batch_size, shuffle=False,\n",
    "                                    collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        # instantiate all training components\n",
    "        model, optimizer, criterion = self._init_training(input_dim=input_dim,\n",
    "                                                          output_dim=output_dim,\n",
    "                                                          src_pad_idx=src_pad_idx,\n",
    "                                                          trg_pad_idx=trg_pad_idx,\n",
    "                                                          device=self.resource.device)\n",
    "        \n",
    "        # train the model on the training set and evaluate after every epoch on the validation set\n",
    "        self._train(model=model,\n",
    "                    train_iterator=train_iterator,\n",
    "                    valid_iterator=valid_iterator,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Evaluation\n",
    "\n",
    "\n",
    "```python\n",
    "class Evaluation(Task):\n",
    "    def __init__(self, test_batch_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = test_batch_size\n",
    "\n",
    "    def _init_model(self, train_config: Dict, input_dim: int, output_dim: int,\n",
    "                    src_pad_idx: int, trg_pad_idx: int) -> nn.Module:\n",
    "        enc = Encoder(input_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['enc_layers'],\n",
    "                      train_config['enc_heads'],\n",
    "                      train_config['enc_pf_dim'],\n",
    "                      train_config['enc_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        dec = Decoder(output_dim,\n",
    "                      train_config['hid_dim'],\n",
    "                      train_config['dec_layers'],\n",
    "                      train_config['dec_heads'],\n",
    "                      train_config['dec_pf_dim'],\n",
    "                      train_config['dec_dropout'],\n",
    "                      self.resource.device)\n",
    "\n",
    "        model = Seq2SeqTransformer(enc, dec, src_pad_idx, trg_pad_idx, self.resource.device).to(self.resource.device)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def _select_best_model_from_sweeps(training_results: List[Dict]) -> Dict:\n",
    "        config = None\n",
    "        best_valid_loss = float('inf')\n",
    "        for sweep in training_results:\n",
    "            if sweep['result']['best_model_metric']['valid_loss'] <= best_valid_loss:\n",
    "                best_valid_loss = sweep['result']['best_model_metric']['valid_loss']\n",
    "                config = sweep['config']\n",
    "        return config\n",
    "\n",
    "    def translate_sentence(self, src_encoded, bos_idx, eos_idx, model, max_len=50):\n",
    "        model.eval()\n",
    "\n",
    "        src_tensor = torch.LongTensor(src_encoded).unsqueeze(0).to(self.resource.device)\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "        trg_indices = [bos_idx]\n",
    "        for i in range(max_len):\n",
    "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(self.resource.device)\n",
    "            trg_mask = model.make_trg_mask(trg_tensor)\n",
    "            with torch.no_grad():\n",
    "                output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "            trg_indices.append(pred_token)\n",
    "            if pred_token == eos_idx:\n",
    "                break\n",
    "\n",
    "        return trg_indices[1:], attention\n",
    "\n",
    "    def calculate_bleu(self, data_encoded, en_tokenizer, model, max_len=50):\n",
    "        trgs = []\n",
    "        pred_trgs = []\n",
    "        bos_idx = en_tokenizer.token_to_id('<bos>')\n",
    "        eos_idx = en_tokenizer.token_to_id('<eos>')\n",
    "\n",
    "        for src, trg in tqdm(data_encoded):\n",
    "\n",
    "            pred_trg, _ = self.translate_sentence(src, bos_idx, eos_idx, model, max_len)\n",
    "\n",
    "            # cut off <eos> token\n",
    "            pred_trg = pred_trg[:-1]\n",
    "\n",
    "            pred_trg_decoded = en_tokenizer.decode(pred_trg)\n",
    "            pred_trgs.append(pred_trg_decoded.split())\n",
    "\n",
    "            trg_decoded = en_tokenizer.decode(trg)\n",
    "            trgs.append(trg_decoded.split())\n",
    "\n",
    "        return bleu_score(pred_trgs, trgs)\n",
    "\n",
    "    def run(self, reduced_results: List[Dict]):\n",
    "        set_seed(self.resource.seed)\n",
    "\n",
    "        config = self._select_best_model_from_sweeps(training_results=reduced_results)\n",
    "\n",
    "        model_state_dict = self.load(name='best_model', task_name='Training', task_unique_config=config)\n",
    "        test_encoded = self.load(name='test_encoded', task_name='DatasetEncoding', task_unique_config=config)\n",
    "        de_tokenizer = self.load(name='de_tokenizer', task_name='TokenizerTraining', task_unique_config=config)\n",
    "        en_tokenizer = self.load(name='en_tokenizer', task_name='TokenizerTraining', task_unique_config=config)\n",
    "\n",
    "        batch_collator = BatchCollator(de_pad_idx=de_tokenizer.token_to_id('<pad>'),\n",
    "                                       en_pad_idx=en_tokenizer.token_to_id('<pad>'),\n",
    "                                       device=self.resource.device)\n",
    "\n",
    "        test_dataset = TranslationDataset(data=test_encoded)\n",
    "\n",
    "        test_iterator = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, collate_fn=batch_collator)\n",
    "\n",
    "        input_dim = de_tokenizer.get_vocab_size()\n",
    "        output_dim = en_tokenizer.get_vocab_size()\n",
    "        src_pad_idx = de_tokenizer.token_to_id('<pad>')\n",
    "        trg_pad_idx = en_tokenizer.token_to_id('<pad>')\n",
    "\n",
    "        model = self._init_model(train_config=config['Training'],\n",
    "                                 input_dim=input_dim,\n",
    "                                 output_dim=output_dim,\n",
    "                                 src_pad_idx=src_pad_idx,\n",
    "                                 trg_pad_idx=src_pad_idx)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "\n",
    "        test_loss = Training.validate_epoch(model=model, iterator=test_iterator, criterion=criterion)\n",
    "        print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n",
    "        bleu = self.calculate_bleu(test_encoded[:50], en_tokenizer, model)\n",
    "        print(f'BLEU score = {bleu * 100:.2f}')\n",
    "\n",
    "        self.save(obj=config, name='best_model_config', type_='json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Flow/Pipeline\n",
    "\n",
    "So far, we have looked into implementing our individual pipeline steps using FluidML's Task class and it was very straightforward.\n",
    "You might be wondering, how to put these tasks together and make them work together as a single pipeline?\n",
    "\n",
    "Thanks to FluidML's TaskSpec API, you can connect these tasks like Lego blocks :)\n",
    "\n",
    "### Task Specifications\n",
    "TaskSpec is a simple wrapper that allows to specify task details and task arguments which will be used during instantiation of the task.\n",
    "Let's go ahead and create specs for all our tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "\n",
    "dataset_loading_task = TaskSpec(task=DatasetLoading, task_kwargs=dataset_loading_params,\n",
    "                                publishes=['train_data', 'valid_data', 'test_data'])\n",
    "\n",
    "tokenizer_training_task = GridTaskSpec(task=TokenizerTraining, gs_config=tokenizer_training_params,\n",
    "                                       expects=['train_data'],\n",
    "                                       publishes=['de_tokenizer', 'en_tokenizer'])\n",
    "\n",
    "dataset_encoding_task = TaskSpec(task=DatasetEncoding,\n",
    "                                 expects=['train_data', 'valid_data', 'test_data', 'de_tokenizer', 'en_tokenizer'],\n",
    "                                 publishes=['train_encoded', 'valid_encoded', 'test_encoded'])\n",
    "\n",
    "train_task = GridTaskSpec(task=Training, gs_config=training_params,\n",
    "                          expects=['train_encoded', 'valid_encoded', 'de_tokenizer', 'en_tokenizer'],\n",
    "                          publishes=['best_model', 'best_model_metric'])\n",
    "\n",
    "evaluate_task = TaskSpec(task=Evaluation, reduce=True, task_kwargs=evaluation_params,\n",
    "                         expects=['best_model_metric'],\n",
    "                         publishes=['best_model_config'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Dependencies\n",
    "After having instantiated all task specs, we utilize the `requires()` method to register dependencies between tasks.\n",
    "\n",
    "Using these task dependencies, FluidML creates a task graph and schedules the task executions considering the dependencies.\n",
    "Further, it automatically collects the saved results from predecessor tasks and makes them available to the next task through the `run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_training_task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c7b984029dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# register dependencies between tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer_training_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_loading_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset_encoding_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer_training_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_loading_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_encoding_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_training_task\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_training_task' is not defined"
     ]
    }
   ],
   "source": [
    "# register dependencies between tasks\n",
    "\n",
    "tokenizer_training_task.requires([dataset_loading_task])\n",
    "dataset_encoding_task.requires([tokenizer_training_task, dataset_loading_task])\n",
    "train_task.requires([dataset_encoding_task, tokenizer_training_task])\n",
    "evaluate_task.requires([train_task])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final list of tasks\n",
    "We pack all these task specs in a list which gets passed to FluidML. `Flow` internally creates the task instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [dataset_loading_task, tokenizer_training_task, dataset_encoding_task, train_task, evaluate_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TaskResource(Resource):\n",
    "    device: str\n",
    "    seed: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_devices(count: Optional[int] = None,\n",
    "                         use_cuda: bool = True) -> List[str]:\n",
    "    count = count if count is not None else multiprocessing.cpu_count()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        devices = [f'cuda:{id_}' for id_ in range(torch.cuda.device_count())]\n",
    "    else:\n",
    "        devices = ['cpu']\n",
    "    factor = int(count / len(devices))\n",
    "    remainder = count % len(devices)\n",
    "    devices = devices * factor + devices[:remainder]\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "base_dir = os.path.join(current_dir, 'seq2seq_experiments')\n",
    "\n",
    "num_workers = 1\n",
    "force = None  # choices [selected, all, None]\n",
    "task_to_execute = None\n",
    "use_cuda = True\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of resources\n",
    "devices = get_balanced_devices(count=num_workers, use_cuda=use_cuda)\n",
    "resources = [TaskResource(device=devices[i], seed=seed) for i in range(num_workers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local file storage used for versioning\n",
    "results_store = MyLocalFileStore(base_dir=base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Swarm(n_dolphins=num_workers,\n",
    "           resources=resources,\n",
    "           results_store=results_store) as swarm:\n",
    "    flow = Flow(swarm=swarm, task_to_execute=task_to_execute, force=force)\n",
    "    flow.run(tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
