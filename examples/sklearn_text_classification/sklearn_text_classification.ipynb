{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fluidml/fluidml/main/logo/fluid_ml_logo.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Text Classification using FluidML and Sklearn**\n",
    "In this notebook, we'll go over some basics of FluidML and implement a complete ML pipeline that performs text classification.\n",
    "Like any other ML pipeline, this usually consists of several tasks:\n",
    "- **Dataset fetching** - Downloads and parses the dataset for text classification\n",
    "- **Dataset pre-processing** - Pre-processes the raw dataset\n",
    "- **Featurization** - Converts the raw sentences to numerical vectors\n",
    "- **Training a classifier** - Trains a logistic regression model\n",
    "- **Evaluation of classifier** - Evaluates the trained model on train/test splits\n",
    "\n",
    "With FluidML, all of these steps are naturally implemented as individual tasks which register their dependencies and are chained together to a task graph. This graph is then executed in parallel by FluidML and all results are returned at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Setup**\n",
    "\n",
    "To run this example, make sure to install FluidML with the additional example requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install fluidml[examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Note 1**: Due to the limitation of multiprocessing and jupyter, we have to import our defined tasks and some helper classes from a separate script. Hence, our task definitions are located in `sklearn_text_classification.py`, which not only implements the tasks but also the entire functionality of this example. So the interested reader can also go ahead and execute the just mentioned script. In order to still make this notebook self-explanatory, we provide Markdown code snippets of the individual task implementations at the place where we would have defined the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fluidml import Task, Flow, TaskSpec\n",
    "\n",
    "from sklearn_text_classification import DatasetFetchTask, PreProcessTask, TFIDFFeaturizeTask, GloveFeaturizeTask, TrainTask, EvaluateTask, ModelSelectionTask\n",
    "from rich import print"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note 2**: If you want to use FluidML's logging capability, please configure a logger using Python's `logging` API. For convenience, we provide a simple utility function which configures a visually appealing logger (using a specific handler from the `rich` library)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fluidml.common.logging import configure_logging\n",
    "configure_logging(level='INFO')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Task Definitions**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1. Dataset Fetching**\n",
    "\n",
    "Let's use HuggingFace's [datasets](https://huggingface.co/datasets) repository to quick get access to a text classification dataset. Specifically, we will use [TREC](https://huggingface.co/datasets/trec) which is a question classification dataset containing ~ 5k labeled questions in training set and ~500 questions in test set. The dataset contains two types of labels: fine and coarse. For simplicity, let's go ahead with coarse, with unique 6 labels.\n",
    "\n",
    "We can implement this dataset collection as a separate task on its own by inheriting from FluidML's Task class. It just has to implement a run() method and publish its results. \n",
    "For simplicity, we can implement this task to publish a nested dictionary. On the outer level, we have different splits and in each split, we will have list of sentences and labels.\n",
    "\n",
    "**Note:** This task publishes a 'raw dataset' as specified in `self.save(dataset_splits, \"raw_dataset\")`. Only the items saved in such a manner can be passed to the downstream tasks.\n",
    "\n",
    "Here is the complete implementation of DatasetFetchTask:\n",
    "\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class DatasetFetchTask(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_split(dataset, split):\n",
    "        if split == \"test\":\n",
    "            return dataset[split]\n",
    "        elif split in [\"train\", \"val\"]:\n",
    "            splitted = list(dataset[\"train\"])\n",
    "            split_index = int(0.7 * len(splitted))\n",
    "            return splitted[:split_index] if split == \"train\" else splitted[split_index:]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sentences_and_labels(dataset) -> Tuple[List[str], List[str]]:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for item in dataset:\n",
    "            sentences.append(item[\"text\"])\n",
    "            labels.append(item[\"coarse_label\"])\n",
    "        return sentences, labels\n",
    "\n",
    "    def run(self):\n",
    "        dataset = load_dataset(\"trec\")\n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "        dataset_splits = {}\n",
    "        for split in splits:\n",
    "            dataset_split = DatasetFetchTask._get_split(dataset, split)\n",
    "            sentences, labels = DatasetFetchTask._get_sentences_and_labels(dataset_split)\n",
    "            split_results = {\"sentences\": sentences, \"labels\": labels}\n",
    "            dataset_splits[split] = split_results\n",
    "        self.save(dataset_splits, \"raw_dataset\")\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2. Dataset Pre-processing:**\n",
    "Now that we have our raw datasets prepared, next, we can apply some pre-processing to clean them up a bit, such as *removing punctuations*, *removing digits* and *making lower case* etc. \n",
    "We will implement this logic into a PreProcessTask. Additionally, this task takes a list of pre-processing steps as parameters.\n",
    "\n",
    "**Note**, this task assumes that the raw dataset is available to it as arguments of run() method. This will be automatically passed by FluidML. \n",
    "Later, when we create the PreProcess Task, we just have to connect the DatasetFetchTask as a predecessor. More on this later, when we model the flow/pipeline.\n",
    "\n",
    "Therefore, PreProcessTask just has to implement its own logic and save the pre-processed sentences. In this way, we can implement the tasks individually (**separation of concerns**)\n",
    "\n",
    "Here is the complete implementation of PreProcessTask:\n",
    "\n",
    "```python\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "class PreProcessTask(Task):\n",
    "    def __init__(self, pre_processing_steps: List[str]):\n",
    "        super().__init__()\n",
    "        self._pre_processing_steps = pre_processing_steps\n",
    "\n",
    "    def _pre_process(self, text: Dict) -> str:\n",
    "        pre_processed_text = text\n",
    "        for step in self._pre_processing_steps:\n",
    "            if step == \"lower_case\":\n",
    "                pre_processed_text = pre_processed_text.lower()\n",
    "            if step == \"remove_punct\":\n",
    "                pre_processed_text = pre_processed_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            if step == \"remove_digits\":\n",
    "                pre_processed_text = re.sub(r\"\\d+\", \"<num>\", pre_processed_text)\n",
    "        return pre_processed_text\n",
    "\n",
    "    def run(self, raw_dataset: Dict):\n",
    "        pre_processed_splits = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            pre_processed_sentences = [self._pre_process(sentence) for sentence in raw_dataset[split][\"sentences\"]]\n",
    "            pre_processed_splits[split] = {\"sentences\": pre_processed_sentences}\n",
    "        self.save(pre_processed_splits, \"pre_processed_dataset\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3. Featurization:**\n",
    "\n",
    "Our datasets are prepared and sentences are pre-processed, so now we can convert these to numerical vectors which can be then fed to classifiers. To this end, we would like to implement two featurizers, namely a TFIDF featurizer and a glove featurizer. They can be implemented as two independent tasks which take preprocessed sentences and save vectorized sentences.\n",
    "\n",
    "You may have guessed the pattern already, this task expects pre_processed dataset, which was produced by PreProcessTask. \n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "\n",
    "class TFIDFFeaturizeTask(Task):\n",
    "    def __init__(self, min_df: int, max_features: int):\n",
    "        super().__init__()\n",
    "        self._min_df = min_df\n",
    "        self._max_features = max_features\n",
    "\n",
    "    def run(self, pre_processed_dataset: Dict):\n",
    "        tfidf_model = TfidfVectorizer(min_df=self._min_df, max_features=self._max_features)\n",
    "        tfidf_model.fit(pre_processed_dataset[\"train\"][\"sentences\"])\n",
    "        featurized_splits = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            tfidf_vectors = tfidf_model.transform(pre_processed_dataset[split][\"sentences\"]).toarray()\n",
    "            featurized_splits[split] = {\"vectors\": tfidf_vectors}\n",
    "        self.save(featurized_splits, \"tfidf_featurized_dataset\")\n",
    "\n",
    "\n",
    "class GloveFeaturizeTask(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self, pre_processed_dataset: Dict):\n",
    "        featurized_splits = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            sentences = [Sentence(sent) for sent in pre_processed_dataset[split][\"sentences\"]]\n",
    "            embedder = DocumentPoolEmbeddings([WordEmbeddings(\"glove\")])\n",
    "            embedder.embed(sentences)\n",
    "            glove_vectors = [sent.embedding.cpu().numpy() for sent in sentences]\n",
    "            glove_vectors = np.array(glove_vectors).reshape(len(glove_vectors), -1)\n",
    "            featurized_splits[split] = {\"vectors\": glove_vectors}\n",
    "        self.save(featurized_splits, \"glove_featurized_dataset\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4. Training a classifier**\n",
    "\n",
    "We are all set to train a simple classifier. For this tutorial, let's stick with a simple logistic regression model from Sklearn.\n",
    "For the inputs, we can stack the glove and tfidf vectors (obtained from featurization task results) and for the targets, we can just use the labels from the raw dataset. In the end, this task returns a trained SKlearn classifier.\n",
    "\n",
    "**Note:** You are not limited just to Sklearn. You can train any kind of model using your favorite library (PyTorch, TensorFlow, Keras, PyTorch Lightning, etc) \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class TrainTask(Task):\n",
    "    def __init__(self, max_iter: int, balanced: str):\n",
    "        super().__init__()\n",
    "        self._max_iter = max_iter\n",
    "        self._class_weight = \"balanced\" if balanced else None\n",
    "\n",
    "    def run(self, raw_dataset: Dict, tfidf_featurized_dataset: Dict, glove_featurized_dataset: Dict):\n",
    "        model = LogisticRegression(max_iter=self._max_iter, class_weight=self._class_weight)\n",
    "        stacked_vectors = np.hstack(\n",
    "            (tfidf_featurized_dataset[\"train\"][\"vectors\"], glove_featurized_dataset[\"train\"][\"vectors\"])\n",
    "        )\n",
    "        model.fit(stacked_vectors, raw_dataset[\"train\"][\"labels\"])\n",
    "        self.save(model, \"trained_model\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **5. Evaluation of classifier**\n",
    "\n",
    "Now, that we have trained a classifier, it is time to evaluate this classifier on all of the dataset splits. This task is straightforward, we will get the featurized dataset splits and the trained model from the results. \n",
    "At the end, EvaluateTask saves a nested dictionary containing classification reports for each of train, val and test splits.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class EvaluateTask(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        raw_dataset: Dict,\n",
    "        tfidf_featurized_dataset: Dict,\n",
    "        glove_featurized_dataset: Dict,\n",
    "        trained_model: LogisticRegression,\n",
    "    ):\n",
    "        evaluation_results = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            stacked_vectors = np.hstack(\n",
    "                (tfidf_featurized_dataset[split][\"vectors\"], glove_featurized_dataset[split][\"vectors\"])\n",
    "            )\n",
    "            predictions = trained_model.predict(stacked_vectors)\n",
    "            report = classification_report(raw_dataset[split][\"labels\"], predictions, output_dict=True)\n",
    "            evaluation_results[split] = {\"classification_report\": report}\n",
    "        self.save(evaluation_results, \"evaluation_results\")\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Create and Run the Pipeline/Task-Graph via FluidML**\n",
    "\n",
    "So far, we have looked into implementing our individual pipeline steps using FluidML's Task class and it was very straightforward.\n",
    "You might be wondering, how to put these tasks together and make them work together as a single pipeline?\n",
    "\n",
    "Thanks to FluidML's TaskSpec API, you can connect these tasks like Lego blocks :)\n",
    "\n",
    "### **1. Instantiate Task Specifications**\n",
    "TaskSpec is a simple wrapper that allows to specify task details task arguments which will be used during instantiation of the task.\n",
    "Let's go ahead and create specs for all our tasks.\n",
    "\n",
    "**Note 2:** We need to pass the `reduce=True` parameter to the ModelSelection task, so that it will get access to *all* previous evaluation results. So, it *reduces* the task graph."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "dataset_fetch_task = TaskSpec(task=DatasetFetchTask)\n",
    "pre_process_task = TaskSpec(task=PreProcessTask, config={\"pre_processing_steps\": [\"lower_case\", \"remove_punct\"]})\n",
    "featurize_task_1 = TaskSpec(task=GloveFeaturizeTask)\n",
    "featurize_task_2 = TaskSpec(task=TFIDFFeaturizeTask, config={\"min_df\": 5, \"max_features\": 2000})\n",
    "train_task = TaskSpec(task=TrainTask, config={\"max_iter\": 50, \"balanced\": True})\n",
    "evaluate_task = TaskSpec(task=EvaluateTask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2. Registering Task Dependencies**\n",
    "More importantly, TaskSpec also provides `requires()` method to specify predecessor tasks which need to be executed before that particular task.\n",
    "\n",
    "For instance, in our example, we would need DatasetFetchTask to be finished before we start to run PreProcessTask. Similarly,\n",
    "PreProcessTask is required for both FeaturizeTasks.\n",
    "\n",
    "Using these task dependencies, FluidML creates a task graph and schedules the tasks considering the dependencies.\n",
    "Not just that, it automatically collects the results from predecessor tasks and makes it available to `run()` method.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dependencies between tasks\n",
    "pre_process_task.requires(dataset_fetch_task)\n",
    "featurize_task_1.requires(pre_process_task)\n",
    "featurize_task_2.requires(pre_process_task)\n",
    "train_task.requires([dataset_fetch_task, featurize_task_1, featurize_task_2])\n",
    "evaluate_task.requires([dataset_fetch_task, featurize_task_1, featurize_task_2, train_task])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3. Creating a Final list of Task Specs**\n",
    "We can just hold all these tasks in a list which we will pass it to FluidML."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all task specs\n",
    "tasks = [\n",
    "    dataset_fetch_task,\n",
    "    pre_process_task,\n",
    "    featurize_task_1,\n",
    "    featurize_task_2,\n",
    "    train_task,\n",
    "    evaluate_task,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **4. Run the Pipeline/Task-Graph using Swarm & Flow**\n",
    "\n",
    "All that is left is to create the `Flow` object, which will handle executing the created task graph in parallel and builds the user defined task specifier graph and the expanded task graph. The latter two can be easily accessed as attributes and visualized using our console rendering utility function. Finally, we execute the expanded task graph by calling `flow.run()`\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create flow\n",
    "flow = Flow(tasks=tasks)\n",
    "# run the pipeline\n",
    "results = flow.run(project_name=\"sklearn_text_classification_example\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **5. Results**\n",
    "We can now go over the results and fetch a task's result using its name, which would give task results and task_config (up until that task in the graph)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"EvaluateTask\"][\"config\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"EvaluateTask\"][\"results\"][\"evaluation_results\"][\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Grid Search:**\n",
    "\n",
    "We can extend this pipeline and include grid search to find hyper-parameter tuning on the whole pipeline.\n",
    "To enable grid search on a particular task, we just have specify `expand` parameter with either `\"product\"` or `\"zip\"`.\n",
    "\n",
    "Depending on the selected grid search expansion method, Flow would expand this task into several task instances with provided combinations of `max_iter` and `balanced`. Namely, `expand=\"product\"` (default) will create 4 task instances for each explicit cross-product parameter combination. Alternatively, `expand=\"zip\"` expands the config parameters by zipping over the different parameter lists, creating 2 task instances for the above example with the combinations `[(50, True), (100, False)]`. When choosing `zip` one has to make sure that all parameter lists are of equal lengths. \n",
    "\n",
    "Any successor tasks (for instance, evaluate task) in the task graph will also be automatically extended. In our example, we have 8 evaluate tasks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Model Selection Task:**\n",
    "\n",
    "Ok that's nice. We have now several evaluation tasks and once we run this task graph through Flow, we will have several evaluation tasks and their results.\n",
    "We can implement a model selection task, which consolidates these results and fetches the best config for the pipeline.\n",
    "\n",
    "```python\n",
    "class ModelSelectionTask(Task):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def run(self, evaluation_results: List[Sweep]):\n",
    "        sorted_results = sorted(\n",
    "            evaluation_results,\n",
    "            key=lambda model_result: model_result.value[\"val\"][\"classification_report\"][\"macro avg\"][\"f1-score\"],\n",
    "            reverse=True,\n",
    "        )\n",
    "        self.save(sorted_results[0].config, \"best_config\")\n",
    "        self.save(sorted_results[0].value, \"best_performance\")\n",
    "```\n",
    "\n",
    "\n",
    "**Note:** Now, we can attach the EvaluationTask as a predecessor to ModelSelectionTask. However, if we do it naively, we would end up with 4 model selection tasks since we have 4 evaluation tasks.\n",
    "So, we just have to specify `reduce=True` so only one instance of model selection task is created and all of the evaluation tasks are attached as parents to it.\n",
    "\n",
    "```python\n",
    "model_selection_task = TaskSpec(task=ModelSelectionTask, reduce=True)\n",
    "model_selection_task.requires([evaluate_task])\n",
    "```\n",
    "Finally, putting all of these together:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "dataset_fetch_task = TaskSpec(task=DatasetFetchTask)\n",
    "pre_process_task = TaskSpec(task=PreProcessTask, config={\"pre_processing_steps\": [\"lower_case\", \"remove_punct\"]})\n",
    "featurize_task_1 = TaskSpec(task=GloveFeaturizeTask)\n",
    "featurize_task_2 = TaskSpec(\n",
    "    task=TFIDFFeaturizeTask, config={\"min_df\": 5, \"max_features\": [1000, 2000]}, expand=\"product\"\n",
    ")\n",
    "train_task = TaskSpec(task=TrainTask, config={\"max_iter\": [50, 100], \"balanced\": [True, False]}, expand=\"product\")\n",
    "evaluate_task = TaskSpec(task=EvaluateTask)\n",
    "model_selection_task = TaskSpec(task=ModelSelectionTask, reduce=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dependencies between tasks\n",
    "pre_process_task.requires(dataset_fetch_task)\n",
    "featurize_task_1.requires(pre_process_task)\n",
    "featurize_task_2.requires(pre_process_task)\n",
    "train_task.requires([dataset_fetch_task, featurize_task_1, featurize_task_2])\n",
    "evaluate_task.requires([dataset_fetch_task, featurize_task_1, featurize_task_2, train_task])\n",
    "model_selection_task.requires(evaluate_task)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [dataset_fetch_task,\n",
    "         pre_process_task,\n",
    "         featurize_task_1, featurize_task_2,\n",
    "         train_task,\n",
    "         evaluate_task,\n",
    "         model_selection_task]\n",
    "\n",
    "flow = Flow(tasks=tasks)\n",
    "\n",
    "# run the pipeline\n",
    "results = flow.run(project_name=\"sklearn_text_classification_example\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Choosing the best Config and best performance**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"ModelSelectionTask\"][\"results\"][\"best_config\"])\n",
    "print(results[\"ModelSelectionTask\"][\"results\"][\"best_performance\"][\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluidml",
   "language": "python",
   "name": "fluidml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}