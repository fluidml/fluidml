{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fluidml/fluidml/blob/main/logo/fluid_ml_logo.png\" width=60 height=60 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial: Text Classification using FluidML and Sklearn**\n",
    "In this notebook, we'll go over some basics of FluidML and implement a complete ML pipeline that performs text classification.\n",
    "Like any other ML pipeline, this usually consists of several steps:\n",
    "- Dataset collection\n",
    "- Dataset pre-processing\n",
    "- Featurization\n",
    "- Training a classifier\n",
    "- Evaluation of classifier\n",
    "\n",
    "With FluidML, all of these steps can naturally be implemented as individual tasks and then later put together as a task graph/flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "\n",
    "**Note**: Due to the limitation of multiprocessing and jupyter, we have to import task definitions from a separate script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fluidml.common import Task, Resource\n",
    "from fluidml.swarm import Swarm\n",
    "from fluidml.flow import Flow, GridTaskSpec, TaskSpec\n",
    "from with_gs import DatasetFetchTask, PreProcessTask, TFIDFFeaturizeTask, GloveFeaturizeTask, TrainTask, EvaluateTask\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Dataset Collection**\n",
    "\n",
    "Let's use HuggingFace's [datasets](https://huggingface.co/datasets) repository to quick get access to a text classification dataset. Specifically, we will use [TREC](https://huggingface.co/datasets/trec) which is a question classification dataset containing ~ 5k labeled questions in training set and ~500 questions in test set. The dataset contains two types of labels: fine and coarse. For simplicity, let's go ahead with coarse, with unique 6 labels.\n",
    "\n",
    "We can implement this dataset collection as a separate task on its own by inheriting from FluidML's Task class. It just has to implement a run() method and returns task results in a dictionary. \n",
    "For simplicity, we can implement this task to return a nested dictionary. On the outer level, we have different splits and in each split, we will have list of sentences and labels.\n",
    "\n",
    "Here is the complete implementation of DatasetFetchTask:\n",
    "\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class DatasetFetchTask(Task):\n",
    "    def __init__(self, name: str, id_: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def _get_split(self, dataset, split):\n",
    "        if split == \"test\":\n",
    "            return dataset[split]\n",
    "        elif split in [\"train\", \"val\"]:\n",
    "            splitted = list(dataset[\"train\"])\n",
    "            split_index = int(0.7 * len(splitted))\n",
    "            return splitted[:split_index] if split == \"train\" else splitted[split_index:]\n",
    "\n",
    "    def _get_sentences_and_labels(self, dataset) -> Tuple[List[str], List[str]]:\n",
    "        sentences = []\n",
    "        labels = []\n",
    "        for item in dataset:\n",
    "            sentences.append(item[\"text\"])\n",
    "            labels.append(item[\"label-coarse\"])\n",
    "        return sentences, labels\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        dataset = load_dataset(\"trec\")\n",
    "        splits = [\"train\", \"val\", \"test\"]\n",
    "        task_results = {}\n",
    "        for split in splits:\n",
    "            dataset_split = self._get_split(dataset, split)\n",
    "            sentences, labels = self._get_sentences_and_labels(dataset_split)\n",
    "            split_results = {\n",
    "                \"sentences\": sentences,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "            task_results[split] = split_results\n",
    "        return task_results\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Dataset Pre-processing:**\n",
    "Now that we have our raw datasets prepared, next, we can apply some pre-processing to clean them up a bit, such as *removing punctuations*, *removing digits* and *making lower case* etc. \n",
    "We will implement this logic into a PreProcessTask. Additionally, this task takes a list of pre-processing steps as parameters.\n",
    "\n",
    "**Note**, this task assumes that the raw sentences are available to it via results dictionary as arguments of run() method. This is automatically ensured by FluidML itself. \n",
    "Therefore, PreProcessTask just has to implement its own logic and return pre-processed sentences as results (**separation of concerns**)\n",
    "\n",
    "Here is the complete implementation of PreProcessTask:\n",
    "\n",
    "```python\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "class PreProcessTask(Task):\n",
    "    def __init__(self, name: str, id_: int, pre_processing_steps: List[str]):\n",
    "        super().__init__(name, id_)\n",
    "        self._pre_processing_steps = pre_processing_steps\n",
    "\n",
    "    def _pre_process(self, text: str) -> str:\n",
    "        pre_processed_text = text\n",
    "        for step in self._pre_processing_steps:\n",
    "            if step == \"lower_case\":\n",
    "                pre_processed_text = pre_processed_text.lower()\n",
    "            if step == \"remove_punct\":\n",
    "                pre_processed_text = pre_processed_text.translate(\n",
    "                    str.maketrans('', '', string.punctuation))\n",
    "            if step == \"remove_digits\":\n",
    "                pre_processed_text = re.sub(\n",
    "                    r\"\\d+\", \"<num>\", pre_processed_text)\n",
    "        return pre_processed_text\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        task_results = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            pre_processed_sentences = [\n",
    "                self._pre_process(sentence) for sentence in results[\"dataset\"][\"result\"][split][\"sentences\"]]\n",
    "            task_results[split] = {\"sentences\": pre_processed_sentences}\n",
    "        return task_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Featurization:**\n",
    "\n",
    "Now that we have our datasets prepared and sentences pre-processed, we can now convert these to numerical vectors which can be then fed to classifiers. To this end, we would like to implement two featurizers namely a TFIDF featurizer and a glove featurizer. They can be implemented as two independent tasks which takes preprocessed sentences and returns vectorized sentences.\n",
    "\n",
    "You may have guessed the pattern already, this task takes takes result from PreProcess task. \n",
    "Note, to fetch results from PreProcessTask, we can simply access it using `results[\"pre_process\"]` where *pre_process* is the task name given to the instance of PreProcessTask. \n",
    "More on this later, when we model the flow/pipeline.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings\n",
    "\n",
    "class TFIDFFeaturizeTask(Task):\n",
    "    def __init__(self, name: str, id_: int, min_df: int, max_features: int):\n",
    "        super().__init__(name, id_)\n",
    "        self._min_df = min_df\n",
    "        self._max_features = max_features\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        tfidf_model = TfidfVectorizer(\n",
    "            min_df=self._min_df, max_features=self._max_features)\n",
    "        tfidf_model.fit(results[\"pre_process\"][\"result\"][\"train\"][\"sentences\"])\n",
    "        task_results = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            tfidf_vectors = tfidf_model.transform(\n",
    "                results[\"pre_process\"][\"result\"][split][\"sentences\"]).toarray()\n",
    "            task_results[split] = {\"vectors\": tfidf_vectors}\n",
    "        return task_results\n",
    "```\n",
    "\n",
    "```python\n",
    "class GloveFeaturizeTask(Task):\n",
    "    def __init__(self, name: str, id_: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        task_results = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            sentences = [Sentence(sent)\n",
    "                         for sent in results[\"pre_process\"][\"result\"][split][\"sentences\"]]\n",
    "            embedder = DocumentPoolEmbeddings([WordEmbeddings(\"glove\")])\n",
    "            embedder.embed(sentences)\n",
    "            glove_vectors = [sent.embedding.cpu().numpy()\n",
    "                             for sent in sentences]\n",
    "            glove_vectors = np.array(glove_vectors).reshape(\n",
    "                len(glove_vectors), -1)\n",
    "            task_results[split] = {\"vectors\": glove_vectors}\n",
    "        return task_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Training a classifier**\n",
    "\n",
    "We are all set to train a simple classifier. For this tutorial, let's stick with a simple logistic regression model from Sklearn.\n",
    "For the inputs, we can stack the glove and tfidf vectors (obtained from featurization task results) and for the targets, we can just use the labels obtained from DatasetFetch task. At the end, this task returns a trained SKlearn classifier.\n",
    "\n",
    "**Note:** You are not limited just to Sklearn. You can train any kind of model using your favorite library (PyTorch, TensorFlow, Keras, PyTorch Lightning, etc) \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class TrainTask(Task):\n",
    "    def __init__(self, name: str, id_: int, max_iter: int, balanced: str):\n",
    "        super().__init__(name, id_)\n",
    "        self._max_iter = max_iter\n",
    "        self._class_weight = \"balanced\" if balanced else None\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        model = LogisticRegression(\n",
    "            max_iter=self._max_iter, class_weight=self._class_weight)\n",
    "        stacked_vectors = np.hstack((results[\"tfidf_featurize\"][\"result\"][\"train\"][\"vectors\"],\n",
    "                                     results[\"glove_featurize\"][\"result\"][\"train\"][\"vectors\"]))\n",
    "        model.fit(stacked_vectors,\n",
    "                  results[\"dataset\"][\"result\"][\"train\"][\"labels\"])\n",
    "        task_results = {\n",
    "            \"model\": model\n",
    "        }\n",
    "        return task_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Evaluation of classifier**\n",
    "\n",
    "Now, that we have trained a classifier, it is time to evaluate this classifier on all of the dataset splits. This task is straightforward, we will get the splits from results of DatasetFetch Task and the model from TrainTask. Finally, EvaluateTask returns a nested dictionary containing classification reports for each of train, val and test splits.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class EvaluateTask(Task):\n",
    "    def __init__(self, name: str, id_: int):\n",
    "        super().__init__(name, id_)\n",
    "\n",
    "    def run(self, results: Dict[str, Any], resource: Resource):\n",
    "        task_results = {}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            stacked_vectors = np.hstack((results[\"tfidf_featurize\"][\"result\"][split][\"vectors\"],\n",
    "                                         results[\"glove_featurize\"][\"result\"][split][\"vectors\"]))\n",
    "            predictions = results[\"train\"][\"result\"][\"model\"].predict(\n",
    "                stacked_vectors)\n",
    "            report = classification_report(\n",
    "                results[\"dataset\"][\"result\"][split][\"labels\"], predictions, output_dict=True)\n",
    "            task_results[split] = {\"classification_report\": report}\n",
    "        return task_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Flow/Pipeline**\n",
    "\n",
    "So far, we have looked into implementing our individual pipeline steps using FluidML's Task class and it was very straightforward.\n",
    "You might be wondering, how to put these tasks together and make them work together as a single pipeline?\n",
    "\n",
    "Thanks to FluidML's TaskSpec API, you can connect these tasks like Lego blocks :)\n",
    "\n",
    "### **Task Specifications**\n",
    "TaskSpec is a simple wrapper that allows to specify task details such as task name and task arguments which will be used during instantiation of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all task specs\n",
    "dataset_fetch_task = TaskSpec(task=DatasetFetchTask, name=\"dataset\")\n",
    "pre_process_task = TaskSpec(task=PreProcessTask, name=\"pre_process\", task_kwargs={\n",
    "                            \"pre_processing_steps\": [\"lower_case\", \"remove_punct\"]})\n",
    "featurize_task_1 = TaskSpec(\n",
    "    task=GloveFeaturizeTask, name=\"glove_featurize\")\n",
    "featurize_task_2 = TaskSpec(\n",
    "    task=TFIDFFeaturizeTask, name=\"tfidf_featurize\", task_kwargs={\"min_df\": 5, \"max_features\": 1000})\n",
    "train_task = TaskSpec(task=TrainTask, name=\"train\",\n",
    "                        task_kwargs={\"max_iter\": 50, \"balanced\": True})\n",
    "evaluate_task = TaskSpec(task=EvaluateTask, name=\"evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task Dependencies**\n",
    "More importantly, TaskSpec also provides `requires()` method to specify predecessor tasks which need to be executed before that particular task.\n",
    "\n",
    "For instance, in our example, we would need DatasetFetchTask to be finished before we start to run PreProcessTask. Similarly,\n",
    "PreProcessTask is required for both FeaturizeTask. We can specify these dependencies on TaskSpec using `requires()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies between tasks\n",
    "pre_process_task.requires([dataset_fetch_task])\n",
    "featurize_task_1.requires([pre_process_task])\n",
    "featurize_task_2.requires([pre_process_task])\n",
    "train_task.requires(\n",
    "    [dataset_fetch_task, featurize_task_1, featurize_task_2])\n",
    "evaluate_task.requires(\n",
    "    [dataset_fetch_task, featurize_task_1, featurize_task_2, train_task])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final list of tasks**\n",
    "We can just hold all these tasks in a list which we will pass it to FluidML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks\n",
    "tasks = [dataset_fetch_task,\n",
    "         pre_process_task,\n",
    "         featurize_task_1, featurize_task_2,\n",
    "         train_task,\n",
    "         evaluate_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Swarm & Flow**\n",
    "Now that we have a final list of tasks which are ready to be run, we just have to create\n",
    "\n",
    "- **Swarm:** which contains several workers which helps to run these tasks parallely. In our example, featurize_task_1 and featurize_task_2 are independent and can be exectued concurrently.\n",
    "- **Flow:** which builds tasks from provided task specifications and creates a task graph which is processed by Swarm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[14:36:27] </span>Swarm scheduling task dataset-<span style=\"color: #000080; font-weight: bold\">0</span>.                                      <a href=\"file:///home/raj/projects/fluidml/fluidml/swarm/swarm.py\"><span style=\"color: #7f7f7f\">swarm.py</span></a><span style=\"color: #7f7f7f\">:105</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f0bc49d8850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000\">'classification_report'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000\">'0'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.8582089552238806</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.8333333333333334</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.8455882352941176</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">138</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'1'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.7945205479452054</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.6170212765957447</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.6946107784431137</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">94</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'2'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.35</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.7777777777777778</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.48275862068965514</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">9</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'3'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.8955223880597015</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.9230769230769231</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.9090909090909091</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">65</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'4'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.9345794392523364</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.8849557522123894</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.9090909090909091</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">113</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'5'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.7878787878787878</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.9629629629629629</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.8666666666666665</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">81</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'accuracy'</span>: <span style=\"color: #000080; font-weight: bold\">0.836</span>,\n",
       "        <span style=\"color: #008000\">'macro avg'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.7701183530599853</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.833188004326522</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.7846343532125619</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">500</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000\">'weighted avg'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000\">'precision'</span>: <span style=\"color: #000080; font-weight: bold\">0.8478047620106426</span>,\n",
       "            <span style=\"color: #008000\">'recall'</span>: <span style=\"color: #000080; font-weight: bold\">0.836</span>,\n",
       "            <span style=\"color: #008000\">'f1-score'</span>: <span style=\"color: #000080; font-weight: bold\">0.8366951980972592</span>,\n",
       "            <span style=\"color: #008000\">'support'</span>: <span style=\"color: #000080; font-weight: bold\">500</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f0a3b611510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Swarm(n_dolphins=2,\n",
    "            refresh_every=10,\n",
    "            return_results=True) as swarm:\n",
    "    flow = Flow(swarm=swarm)\n",
    "    results = flow.run(tasks)\n",
    "print(results[\"evaluate\"][\"result\"][\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/fluidml/fluidml/blob/main/logo/fluid_ml_logo.png\" width=60 height=60 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
